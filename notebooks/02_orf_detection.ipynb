{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af18c4ae",
   "metadata": {},
   "source": [
    "# Advanced Hybrid ORF Detection - Stage 2\n",
    "\n",
    "## Our Algorithm Architecture:\n",
    "1. **Universal ORF Detection** - Find all potential genes\n",
    "2. **Self-Training** - Learn species-specific patterns  \n",
    "3. **Multi-Evidence Scoring** - Combine traditional methods\n",
    "4. **Conflict Resolution** - Optimize overlapping predictions\n",
    "\n",
    "## Goal:\n",
    "Build a unsupervosed gene predictor that adapts to bacterial genome (maybe archeal as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1387c",
   "metadata": {},
   "source": [
    "1. Interpolated Markov Models (IMMs)\n",
    "\n",
    "GLIMMER uses interpolated Markov model to identify coding regions, typically finding 98-99% of all relatively long protein coding genes GLIMMER - Wikipedia\n",
    "Variable-length context models that adapt based on data availability\n",
    "\n",
    "2. Ribosome Binding Site (RBS) Detection\n",
    "\n",
    "Position weight matrix (PWM) that scores any potential RBS, using Gibbs-sampler to find RBS motifs in iterative fashion Identifying bacterial genes and endosymbiont DNA with Glimmer | Bioinformatics | Oxford Academic\n",
    "Critical for accurate start site prediction\n",
    "\n",
    "3. Reverse ORF Scanning\n",
    "\n",
    "Scoring of ORF in GLIMMER 3.0 is done in reverse order starting from stop codon and moves back towards the start codon GLIMMER - Wikipedia\n",
    "More accurate identification of coding regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7b8cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced ORF detection ready\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND GLOBAL VARIABLES\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from Bio.Seq import Seq\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "sys.path.insert(0, '..')  \n",
    "\n",
    "\n",
    "from src.data_management import (\n",
    "    load_genome_sequence,\n",
    "    load_reference_genes_from_gff,\n",
    "    get_fasta_path,\n",
    "    get_gff_path,\n",
    "    TEST_GENOMES  \n",
    ")\n",
    "from src.config import MIN_ORF_LENGTH,START_SELECTION_WEIGHTS,SCORE_WEIGHTS,LENGTH_REFERENCE_BP,MIN_ORF_LENGTH, START_CODON_WEIGHTS\n",
    "from src.cache import load_cache\n",
    "from src.comparative_analysis import compare_orfs_to_reference\n",
    "\n",
    "\n",
    "print(\"Advanced ORF detection ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0375337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from C:\\Users\\User\\Desktop\\Bacterial Gene Prediction & Comparison Project\\data\\processed\\cached_orfs.pkl...\n",
      "Loaded 15 cached genomes\n"
     ]
    }
   ],
   "source": [
    "genome_id = \"NC_000913.3\"\n",
    "cached_data=load_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7366954",
   "metadata": {},
   "source": [
    "# stage 1- Universal ORF Detection\n",
    "- ATG starts: High confidence\n",
    "- GTG starts: Medium confidence \n",
    "- TTG starts: Lower confidence\n",
    "- ribosome binding site detection\n",
    "**Integration Strategy:**\n",
    "Weight final gene scores by start codon reliability to reduce false positives from rare start codons while maintaining sensitivity for real genes with unusual starts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f296be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known RBS motifs - just use these directly\n",
    "KNOWN_RBS_MOTIFS = [\n",
    "    \"AGGAGG\",\n",
    "    \"GGAGG\", \n",
    "    \"AGGAG\",\n",
    "    \"GAGG\",\n",
    "    \"AGGA\",\n",
    "    \"GGAG\"\n",
    "]\n",
    "\n",
    "def find_purine_rich_regions(sequence, min_length=4, min_purine_content=0.6):\n",
    "    \"\"\"\n",
    "    Find purine-rich regions (A and G rich) in sequence.\n",
    "    \"\"\"\n",
    "    purine_regions = []\n",
    "    \n",
    "    for start in range(len(sequence)):\n",
    "        for length in range(min_length, min(9, len(sequence) - start + 1)):\n",
    "            subseq = sequence[start:start + length]\n",
    "            \n",
    "            purines = subseq.count('A') + subseq.count('G')\n",
    "            purine_fraction = purines / length\n",
    "            \n",
    "            if purine_fraction >= min_purine_content:\n",
    "                purine_regions.append({\n",
    "                    'sequence': subseq,\n",
    "                    'start': start,\n",
    "                    'end': start + length,\n",
    "                    'purine_content': purine_fraction,\n",
    "                    'length': length\n",
    "                })\n",
    "    \n",
    "    return purine_regions\n",
    "\n",
    "def evaluate_spacing_score(spacing):\n",
    "    \"\"\"\n",
    "    Evaluate spacing between SD sequence and start codon.\n",
    "    Optimal spacing is 6-10 nucleotides.\n",
    "    \"\"\"\n",
    "    if 6 <= spacing <= 8:\n",
    "        return 3.0  # Optimal\n",
    "    elif 5 <= spacing <= 10:\n",
    "        return 2.5  # Very good\n",
    "    elif 4 <= spacing <= 12:\n",
    "        return 1.5  # Good\n",
    "    elif 3 <= spacing <= 14:\n",
    "        return 1.0  # Acceptable\n",
    "    else:\n",
    "        return 0.2  # Poor\n",
    "\n",
    "def score_motif_similarity(sequence):\n",
    "    \"\"\"\n",
    "    Score how similar a sequence is to known RBS motifs.\n",
    "    No padding, no PWM - just direct comparison.\n",
    "    \"\"\"\n",
    "    best_score = 0.0\n",
    "    best_motif = None\n",
    "    \n",
    "    for motif in KNOWN_RBS_MOTIFS:\n",
    "        # Try different alignments\n",
    "        for offset in range(max(len(sequence), len(motif))):\n",
    "            matches = 0\n",
    "            total_positions = 0\n",
    "            \n",
    "            # Compare overlapping region\n",
    "            for i in range(len(sequence)):\n",
    "                motif_pos = i + offset\n",
    "                if 0 <= motif_pos < len(motif):\n",
    "                    total_positions += 1\n",
    "                    if sequence[i] == motif[motif_pos]:\n",
    "                        matches += 1\n",
    "            \n",
    "            if total_positions > 0:\n",
    "                similarity = matches / total_positions\n",
    "                \n",
    "                # Weight by overlap length and motif quality\n",
    "                overlap_length = total_positions\n",
    "                motif_weight = len(motif) / 6.0  # AGGAGG gets weight 1.0\n",
    "                \n",
    "                score = similarity * overlap_length * motif_weight\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_motif = motif\n",
    "    \n",
    "    return best_score, best_motif\n",
    "\n",
    "def predict_rbs_simple(sequence, orf, upstream_length=20):\n",
    "    \"\"\"\n",
    "    Simple RBS prediction: purine-rich + spacing + direct motif comparison.\n",
    "    \"\"\"\n",
    "    start_pos = orf['start']\n",
    "    \n",
    "    if start_pos < upstream_length:\n",
    "        return {\n",
    "            'rbs_score': -5.0,\n",
    "            'spacing_score': 0.0,\n",
    "            'motif_score': 0.0,\n",
    "            'best_sequence': None,\n",
    "            'best_motif': None,\n",
    "            'spacing': 0,\n",
    "            'position': 0\n",
    "        }\n",
    "\n",
    "    # Translation initiation region (20 bases upstream)\n",
    "    upstream_start = start_pos - upstream_length\n",
    "    upstream_seq = sequence[upstream_start:start_pos]\n",
    "    \n",
    "    # Find purine-rich regions\n",
    "    purine_regions = find_purine_rich_regions(upstream_seq, min_length=4, min_purine_content=0.6)\n",
    "    \n",
    "    best_score = -5.0\n",
    "    best_prediction = None\n",
    "    \n",
    "    # Evaluate each purine-rich region\n",
    "    for region in purine_regions:\n",
    "        sd_candidate = region['sequence']\n",
    "        \n",
    "        spacing = len(upstream_seq) - region['end']\n",
    "        \n",
    "        if spacing < 4 or spacing > 12:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        spacing_score = evaluate_spacing_score(spacing)\n",
    "        \n",
    "        motif_score, best_motif = score_motif_similarity(sd_candidate)\n",
    "        \n",
    "        purine_bonus = (region['purine_content'] - 0.6) * 2.0\n",
    "        \n",
    "        # Combined score\n",
    "        combined_score = (\n",
    "            spacing_score * 2.0 +    # Spacing is critical\n",
    "            motif_score * 1.5 +      # Direct motif similarity  \n",
    "            purine_bonus             # Higher purine content\n",
    "        )\n",
    "        \n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_prediction = {\n",
    "                'rbs_score': combined_score,\n",
    "                'spacing_score': spacing_score,\n",
    "                'motif_score': motif_score,\n",
    "                'best_sequence': sd_candidate,\n",
    "                'best_motif': best_motif,\n",
    "                'spacing': spacing,\n",
    "                'position': region['start'],\n",
    "                'purine_content': region['purine_content'],\n",
    "                'length': region['length']\n",
    "            }\n",
    "    \n",
    "    return best_prediction or {\n",
    "        'rbs_score': -5.0,\n",
    "        'spacing_score': 0.0,\n",
    "        'motif_score': 0.0,\n",
    "        'best_sequence': None,\n",
    "        'best_motif': None,\n",
    "        'spacing': 0,\n",
    "        'position': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af0376a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orfs_candidates(sequence, min_length):\n",
    "    \"\"\"\n",
    "    ORF detection with dual coordinate system and immediate RBS calculation.\n",
    "    \n",
    "    Returns ORFs with:\n",
    "    - 'start', 'end': coordinates in the RC sequence (for scoring)\n",
    "    - 'genome_start', 'genome_end': forward-strand coordinates (for validation)\n",
    "    - 'sequence': RC sequence (for scoring)\n",
    "    - 'rbs_score', 'rbs_motif', 'rbs_spacing', 'rbs_sequence': RBS features\n",
    "    \"\"\"\n",
    "    start_codons = {'ATG', 'GTG', 'TTG'}\n",
    "    stop_codons = {'TAA', 'TAG', 'TGA'}\n",
    "    orfs = []\n",
    "    \n",
    "    sequences = [\n",
    "        ('forward', sequence),\n",
    "        ('reverse', str(Seq(sequence).reverse_complement()))\n",
    "    ]\n",
    "    seq_len = len(sequence)\n",
    "\n",
    "    print(\"Detecting ORFs and calculating RBS...\")\n",
    "\n",
    "    for strand_name, seq in sequences:\n",
    "        for frame in range(3):\n",
    "            active_starts = [] \n",
    "            for i in range(frame, len(seq) - 2, 3):\n",
    "                codon = seq[i:i+3]\n",
    "                \n",
    "                if len(codon) != 3:  \n",
    "                    break\n",
    "                \n",
    "                if codon in start_codons:\n",
    "                    active_starts.append((i, codon))\n",
    "                    \n",
    "                elif codon in stop_codons and active_starts:\n",
    "                    for start_pos, start_codon in active_starts:\n",
    "                        orf_length = i + 3 - start_pos\n",
    "                        if orf_length >= min_length:\n",
    "                            # Create ORF\n",
    "                            if strand_name == 'forward':\n",
    "                                orf = {\n",
    "                                    'start': start_pos + 1,\n",
    "                                    'end': i + 3,\n",
    "                                    'genome_start': start_pos + 1,\n",
    "                                    'genome_end': i + 3,\n",
    "                                    'length': orf_length,\n",
    "                                    'frame': frame,\n",
    "                                    'strand': 'forward',\n",
    "                                    'start_codon': start_codon,\n",
    "                                    'sequence': seq[start_pos:i+3]\n",
    "                                }\n",
    "                            else:  # reverse strand\n",
    "                                orf = {\n",
    "                                    'start': start_pos + 1,\n",
    "                                    'end': i + 3,\n",
    "                                    'genome_start': seq_len - (i + 3) + 1,\n",
    "                                    'genome_end': seq_len - start_pos,\n",
    "                                    'length': orf_length,\n",
    "                                    'frame': frame,\n",
    "                                    'strand': 'reverse',\n",
    "                                    'start_codon': start_codon,\n",
    "                                    'sequence': seq[start_pos:i+3]  \n",
    "                                }\n",
    "                            \n",
    "                            # Calculate RBS\n",
    "                            rbs_result = predict_rbs_simple(seq, orf, upstream_length=20)\n",
    "                            orf['rbs_score'] = rbs_result['rbs_score']\n",
    "                            orf['rbs_motif'] = rbs_result.get('best_motif')\n",
    "                            orf['rbs_spacing'] = rbs_result.get('spacing', 0)\n",
    "                            orf['rbs_sequence'] = rbs_result.get('best_sequence')\n",
    "                            \n",
    "                            orfs.append(orf)\n",
    "                    active_starts = []\n",
    "    \n",
    "    print(f\"Complete: {len(orfs):,} ORFs detected with RBS scores\")\n",
    "    return orfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390a3ad",
   "metadata": {},
   "source": [
    "## Phase 2: Self-Training and Advanced Scoring\n",
    "\n",
    "**Challenge:** 176,315 ORF candidates → ~4,300 real genes  \n",
    "**Solution:** Implement Glimmer-style advanced methods\n",
    "\n",
    "### Methods to Implement:\n",
    "1. **Self-training on long ORFs** - Build species-specific models\n",
    "2. **Interpolated Markov Models** - Score coding potential  \n",
    "3. **RBS (Ribosome Binding Site) detection** - Find translation signals\n",
    "4. **Codon usage bias analysis** - Species-specific patterns\n",
    "\n",
    "### Strategy:\n",
    "Use the longest, most confident ORFs to train models, then score all candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b16db91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING SET SELECTION: possible strategies\n",
    "def calculate_amino_acid_entropy(orf):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of amino acid composition.\n",
    "    Uses cached value if already calculated.\n",
    "    \n",
    "    Lower entropy = biased distribution = likely real gene\n",
    "    Higher entropy = uniform distribution = likely random ORF\n",
    "    \n",
    "    Args:\n",
    "        orf (dict): ORF dictionary with 'sequence' key\n",
    "        \n",
    "    Returns:\n",
    "        float: Entropy value\n",
    "               Returns 999.0 if translation fails\n",
    "               \n",
    "    Side effects:\n",
    "        Caches result in orf['aa_entropy'] for reuse\n",
    "    \"\"\"\n",
    "    # Return cached value if available\n",
    "    if 'aa_entropy' in orf:\n",
    "        return orf['aa_entropy']\n",
    "    \n",
    "    try:\n",
    "        protein = str(Seq(orf['sequence']).translate(to_stop=True))\n",
    "        \n",
    "        # Need sufficient length for reliable statistics\n",
    "        if len(protein) < 10:\n",
    "            orf['aa_entropy'] = 999.0\n",
    "            return 999.0\n",
    "        \n",
    "        # Count amino acid frequencies\n",
    "        aa_counts = Counter(protein)\n",
    "        total = len(protein)\n",
    "        \n",
    "        # Calculate Shannon entropy: H = -Σ(p_i × log₂(p_i))\n",
    "        entropy = 0.0\n",
    "        for count in aa_counts.values():\n",
    "            if count > 0:\n",
    "                freq = count / total\n",
    "                entropy -= freq * math.log2(freq)\n",
    "        \n",
    "        orf['aa_entropy'] = entropy\n",
    "        return entropy\n",
    "        \n",
    "    except Exception:\n",
    "        orf['aa_entropy'] = 999.0\n",
    "        return 999.0\n",
    "\n",
    "def select_training_glimmer_3(all_orfs, min_length=300, \n",
    "                               max_entropy=4.2, max_training_size=2000):\n",
    "    \"\"\"\n",
    "    GLIMMER 3.0 strategy with entropy filtering.\n",
    "    \"\"\"\n",
    "    long_orfs = [orf for orf in all_orfs if orf['length'] >= min_length]\n",
    "    \n",
    "    # Calculate entropy for all ORFs (function handles caching internally)\n",
    "    for orf in long_orfs:\n",
    "        calculate_amino_acid_entropy(orf)\n",
    "    \n",
    "    # Filter by entropy\n",
    "    entropy_filtered = [orf for orf in long_orfs if orf['aa_entropy'] <= max_entropy]\n",
    "    entropy_filtered.sort(key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    # Select non-overlapping\n",
    "    training_set = []\n",
    "    covered_intervals = []\n",
    "    \n",
    "    for orf in entropy_filtered:\n",
    "        start = orf.get('genome_start', orf['start'])\n",
    "        end = orf.get('genome_end', orf['end'])\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "        \n",
    "        overlaps = any(not (end < cov_start or start > cov_end) \n",
    "                      for cov_start, cov_end in covered_intervals)\n",
    "        \n",
    "        if not overlaps:\n",
    "            training_set.append(orf)\n",
    "            covered_intervals.append((start, end))\n",
    "            if max_training_size is not None and len(training_set) >= max_training_size:\n",
    "                break\n",
    "    \n",
    "    return training_set\n",
    "\n",
    "def select_training_glimmer(all_orfs, min_length=300, max_training_size=2000):\n",
    "    \"\"\"\n",
    "    GLIMMER Pure - optimized (no entropy needed).\n",
    "    Robust max_training_size enforcement (stop + final truncation).\n",
    "    \"\"\"\n",
    "    long_orfs = [orf for orf in all_orfs if orf['length'] >= min_length]\n",
    "    long_orfs.sort(key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    training_set = []\n",
    "    covered_intervals = []\n",
    "    \n",
    "    for orf in long_orfs:\n",
    "        start = orf.get('genome_start', orf['start'])\n",
    "        end = orf.get('genome_end', orf['end'])\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "        \n",
    "        overlaps = False\n",
    "        for cov_start, cov_end in covered_intervals:\n",
    "            if not (end < cov_start or start > cov_end):\n",
    "                overlaps = True\n",
    "                break\n",
    "        \n",
    "        if not overlaps:\n",
    "            training_set.append(orf)\n",
    "            covered_intervals.append((start, end))\n",
    "            if max_training_size is not None and len(training_set) >= max_training_size:\n",
    "                break\n",
    "    \n",
    "    return training_set\n",
    "\n",
    "def select_training_flexible(all_orfs, \n",
    "                             target_size=500, \n",
    "                             min_length=300, \n",
    "                             max_length=2400, \n",
    "                             max_overlap_fraction=0.3,\n",
    "                             use_entropy_filter=True, \n",
    "                             max_entropy=4.2,\n",
    "                             prefer_atg=True):\n",
    "    \"\"\"\n",
    "    Flexible training set selection with entropy filtering and controlled overlap.\n",
    "    \n",
    "    This strategy allows some overlap between ORFs (unlike pure Glimmer) and\n",
    "    prioritizes ATG start codons when available.\n",
    "    \n",
    "    Args:\n",
    "        all_orfs (list): All detected ORFs\n",
    "        target_size (int): Target number of training ORFs (default: 500)\n",
    "        min_length (int): Minimum ORF length in bp (default: 300)\n",
    "        max_length (int): Maximum ORF length in bp (default: 2400)\n",
    "        max_overlap_fraction (float): Maximum allowed overlap as fraction of ORF length (default: 0.3)\n",
    "        use_entropy_filter (bool): Whether to filter by amino acid entropy (default: True)\n",
    "        max_entropy (float): Maximum amino acid entropy if filtering (default: 4.2)\n",
    "        prefer_atg (bool): Prioritize ATG start codons (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        list: Selected training ORFs\n",
    "    \"\"\"\n",
    "    # Filter by length\n",
    "    filtered = [orf for orf in all_orfs \n",
    "                if min_length <= orf['length'] <= max_length]\n",
    "    \n",
    "    # Apply entropy filter if requested\n",
    "    if use_entropy_filter:\n",
    "        for orf in filtered:\n",
    "            calculate_amino_acid_entropy(orf)\n",
    "        filtered = [orf for orf in filtered if orf['aa_entropy'] <= max_entropy]\n",
    "    \n",
    "    # Sort candidates: ATG first if preferred, then by length\n",
    "    if prefer_atg:\n",
    "        atg_orfs = [orf for orf in filtered if orf.get('start_codon') == 'ATG']\n",
    "        non_atg_orfs = [orf for orf in filtered if orf.get('start_codon') != 'ATG']\n",
    "        \n",
    "        atg_orfs.sort(key=lambda x: x['length'], reverse=True)\n",
    "        non_atg_orfs.sort(key=lambda x: x['length'], reverse=True)\n",
    "        \n",
    "        candidates = atg_orfs + non_atg_orfs\n",
    "    else:\n",
    "        candidates = sorted(filtered, key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    for orf in candidates:\n",
    "        # Normalize coordinates\n",
    "        orf_start = orf.get('genome_start', orf['start'])\n",
    "        orf_end = orf.get('genome_end', orf['end'])\n",
    "        if orf_start > orf_end:\n",
    "            orf_start, orf_end = orf_end, orf_start\n",
    "        \n",
    "        orf_strand = orf.get('strand', 'forward')\n",
    "        orf_length = orf['length']\n",
    "        \n",
    "        # Check overlap with already selected ORFs on same strand\n",
    "        max_overlap = 0.0\n",
    "        for sel in selected:\n",
    "            # Only check same-strand overlaps\n",
    "            if sel.get('strand', 'forward') != orf_strand:\n",
    "                continue\n",
    "            \n",
    "            # Normalize selected ORF coordinates\n",
    "            sel_start = sel.get('genome_start', sel['start'])\n",
    "            sel_end = sel.get('genome_end', sel['end'])\n",
    "            if sel_start > sel_end:\n",
    "                sel_start, sel_end = sel_end, sel_start\n",
    "            \n",
    "            # Calculate overlap\n",
    "            overlap_bp = max(0, min(orf_end, sel_end) - max(orf_start, sel_start) + 1)\n",
    "            overlap_frac = overlap_bp / orf_length\n",
    "            max_overlap = max(max_overlap, overlap_frac)\n",
    "        \n",
    "        # Accept ORF if overlap is acceptable\n",
    "        if max_overlap <= max_overlap_fraction:\n",
    "            selected.append(orf)\n",
    "            \n",
    "            # Stop when target size reached\n",
    "            if len(selected) >= target_size:\n",
    "                break\n",
    "    \n",
    "    return selected\n",
    "\n",
    "def extract_intergenic_regions(sequence, training_orfs, buffer=50, min_length=150):\n",
    "    \"\"\"\n",
    "    Extract intergenic regions using high-confidence genes (Glimmer approach).\n",
    "    Returns both concatenated sequence and coordinates.\n",
    "    \"\"\"\n",
    "    gene_regions = []\n",
    "    for orf in training_orfs:\n",
    "        start = orf.get('genome_start', orf['start'])\n",
    "        end = orf.get('genome_end', orf['end'])\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "        gene_regions.append((max(1, start-buffer), min(len(sequence), end+buffer)))\n",
    "    \n",
    "    # Merge overlapping regions\n",
    "    merged = []\n",
    "    for s, e in sorted(gene_regions):\n",
    "        if merged and s <= merged[-1][1]:\n",
    "            merged[-1] = (merged[-1][0], max(merged[-1][1], e))\n",
    "        else:\n",
    "            merged.append((s, e))\n",
    "    \n",
    "    # Extract gaps\n",
    "    intergenic_seqs = []\n",
    "    intergenic_coords = []\n",
    "    last_end = 1\n",
    "    for s, e in merged:\n",
    "        if s - last_end >= min_length:\n",
    "            intergenic_coords.append((last_end, s-1))\n",
    "            intergenic_seqs.append(sequence[last_end-1:s-1])\n",
    "        last_end = e + 1\n",
    "    if len(sequence) - last_end + 1 >= min_length:\n",
    "        intergenic_coords.append((last_end, len(sequence)))\n",
    "        intergenic_seqs.append(sequence[last_end-1:])\n",
    "    \n",
    "    concatenated = ''.join(intergenic_seqs)\n",
    "    return concatenated, intergenic_coords\n",
    "\n",
    "def extract_non_orf_regions_conservative(sequence, all_orfs, min_rbs_threshold=3.0, min_length=150):\n",
    "    \"\"\"\n",
    "    Extract non-ORF regions using RBS-filtering.\n",
    "    Returns both concatenated sequence and coordinates.\n",
    "    \"\"\"\n",
    "    filtered = [orf for orf in all_orfs if orf.get('rbs_score', 0) >= min_rbs_threshold]\n",
    "    occupied = []\n",
    "    for orf in filtered:\n",
    "        start = orf.get('genome_start', orf['start'])\n",
    "        end = orf.get('genome_end', orf['end'])\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "        occupied.append((start, end))\n",
    "    \n",
    "    merged = []\n",
    "    for s, e in sorted(occupied):\n",
    "        if merged and s <= merged[-1][1]:\n",
    "            merged[-1] = (merged[-1][0], max(merged[-1][1], e))\n",
    "        else:\n",
    "            merged.append((s, e))\n",
    "    \n",
    "    non_orf_seqs = []\n",
    "    non_orf_coords = []\n",
    "    last_end = 1\n",
    "    for s, e in merged:\n",
    "        if s - last_end >= min_length:\n",
    "            non_orf_coords.append((last_end, s-1))\n",
    "            non_orf_seqs.append(sequence[last_end-1:s-1])\n",
    "        last_end = e + 1\n",
    "    if len(sequence) - last_end + 1 >= min_length:\n",
    "        non_orf_coords.append((last_end, len(sequence)))\n",
    "        non_orf_seqs.append(sequence[last_end-1:])\n",
    "    \n",
    "    concatenated = ''.join(non_orf_seqs)\n",
    "    return concatenated, non_orf_coords\n",
    "\n",
    "\n",
    "def extract_all_non_orf_regions(sequence, all_orfs, min_length=150):\n",
    "    \"\"\"\n",
    "    Extract all non-ORF regions (no filtering).\n",
    "    Returns both concatenated sequence and coordinates.\n",
    "    \"\"\"\n",
    "    occupied = []\n",
    "    for orf in all_orfs:\n",
    "        start = orf.get('genome_start', orf['start'])\n",
    "        end = orf.get('genome_end', orf['end'])\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "        occupied.append((start, end))\n",
    "    \n",
    "    merged = []\n",
    "    for s, e in sorted(occupied):\n",
    "        if merged and s <= merged[-1][1]:\n",
    "            merged[-1] = (merged[-1][0], max(merged[-1][1], e))\n",
    "        else:\n",
    "            merged.append((s, e))\n",
    "    \n",
    "    non_orf_seqs = []\n",
    "    non_orf_coords = []\n",
    "    last_end = 1\n",
    "    for s, e in merged:\n",
    "        if s - last_end >= min_length:\n",
    "            non_orf_coords.append((last_end, s-1))\n",
    "            non_orf_seqs.append(sequence[last_end-1:s-1])\n",
    "        last_end = e + 1\n",
    "    if len(sequence) - last_end + 1 >= min_length:\n",
    "        non_orf_coords.append((last_end, len(sequence)))\n",
    "        non_orf_seqs.append(sequence[last_end-1:])\n",
    "    \n",
    "    concatenated = ''.join(non_orf_seqs)\n",
    "    return concatenated, non_orf_coords\n",
    "\n",
    "def merge_intervals(intervals):\n",
    "    \"\"\"Merge overlapping intervals.\"\"\"\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals = sorted(intervals)\n",
    "    merged = [intervals[0]]\n",
    "    for s, e in intervals[1:]:\n",
    "        if s <= merged[-1][1]:\n",
    "            merged[-1] = (merged[-1][0], max(merged[-1][1], e))\n",
    "        else:\n",
    "            merged.append((s, e))\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7a6587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Test WITHOUT entropy\\nsummary_no_entropy, results_no_entropy = run_genome_test_precomputed_sequential(TEST_GENOMES, cached_data, use_entropy=False)\\n# Test WITH entropy\\nsummary_with_entropy, results_with_entropy = run_genome_test_precomputed_sequential(TEST_GENOMES, cached_data, use_entropy=True)\\n# Test INTERGENIC\\nsummary_df, results_df = run_intergenic_test_precomputed_sequential(TEST_GENOMES, cached_data)\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_intergenic_purity_fast(intergenic_regions, ref_genes_df, genome_length, method_name):\n",
    "    \"\"\"Fast purity check using merged gene intervals for linear scan.\"\"\"\n",
    "    if not intergenic_regions:\n",
    "        return {\n",
    "            'method': method_name,\n",
    "            'total_regions': 0,\n",
    "            'pure_regions': 0,\n",
    "            'contaminated_regions': 0,\n",
    "            'total_bp': 0,\n",
    "            'pure_bp': 0,\n",
    "            'contaminated_bp': 0,\n",
    "            'region_purity': 0.0,\n",
    "            'bp_purity': 0.0,\n",
    "            'pct_of_genome': 0.0\n",
    "        }\n",
    "\n",
    "    # 1. Merge overlapping gene intervals\n",
    "    gene_intervals = sorted([(int(row['start']), int(row['end'])) for _, row in ref_genes_df.iterrows()])\n",
    "    merged_genes = []\n",
    "    for s, e in gene_intervals:\n",
    "        if merged_genes and s <= merged_genes[-1][1]:\n",
    "            merged_genes[-1] = (merged_genes[-1][0], max(merged_genes[-1][1], e))\n",
    "        else:\n",
    "            merged_genes.append((s, e))\n",
    "\n",
    "    # 2. Scan intergenic regions against merged genes\n",
    "    pure_regions = 0\n",
    "    contaminated_regions = 0\n",
    "    pure_bp = 0\n",
    "    contaminated_bp = 0\n",
    "\n",
    "    g_idx = 0\n",
    "    for inter_start, inter_end in intergenic_regions:\n",
    "        region_length = inter_end - inter_start + 1\n",
    "        total_bp_in_region = region_length\n",
    "        region_pure_bp = 0\n",
    "        overlap_found = False\n",
    "\n",
    "        region_start = inter_start\n",
    "        while g_idx < len(merged_genes) and merged_genes[g_idx][1] < inter_start:\n",
    "            g_idx += 1\n",
    "        temp_idx = g_idx\n",
    "        while temp_idx < len(merged_genes) and merged_genes[temp_idx][0] <= inter_end:\n",
    "            overlap_found = True\n",
    "            overlap_start = max(region_start, merged_genes[temp_idx][0])\n",
    "            overlap_end = min(inter_end, merged_genes[temp_idx][1])\n",
    "            region_pure_bp += overlap_start - region_start  # add pure part before overlap\n",
    "            region_start = overlap_end + 1\n",
    "            temp_idx += 1\n",
    "        if region_start <= inter_end:\n",
    "            region_pure_bp += inter_end - region_start + 1\n",
    "\n",
    "        pure_bp += region_pure_bp\n",
    "        contaminated_bp += total_bp_in_region - region_pure_bp\n",
    "        if overlap_found:\n",
    "            contaminated_regions += 1\n",
    "        else:\n",
    "            pure_regions += 1\n",
    "\n",
    "    total_regions = len(intergenic_regions)\n",
    "    total_bp = sum(end - start + 1 for start, end in intergenic_regions)\n",
    "\n",
    "    region_purity = pure_regions / total_regions * 100 if total_regions else 0\n",
    "    bp_purity = pure_bp / total_bp * 100 if total_bp else 0\n",
    "    pct_of_genome = total_bp / genome_length * 100 if genome_length else 0\n",
    "\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'total_regions': total_regions,\n",
    "        'pure_regions': pure_regions,\n",
    "        'contaminated_regions': contaminated_regions,\n",
    "        'total_bp': total_bp,\n",
    "        'pure_bp': pure_bp,\n",
    "        'contaminated_bp': contaminated_bp,\n",
    "        'region_purity': region_purity,\n",
    "        'bp_purity': bp_purity,\n",
    "        'pct_of_genome': pct_of_genome\n",
    "    }\n",
    "\n",
    "def process_single_genome_intergenic(genome_id, cached_data):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        genome_data = cached_data.get(genome_id)\n",
    "        if genome_data is None:\n",
    "            raise ValueError(f\"No precomputed ORFs found for {genome_id}\")\n",
    "\n",
    "        all_orfs = genome_data['orfs']\n",
    "\n",
    "        all_orfs = genome_data['orfs']\n",
    "        gff_path = get_gff_path(genome_id)\n",
    "        fasta_path = get_fasta_path(genome_id)\n",
    "        genome_sequence=load_genome_sequence(fasta_path)\n",
    "        genome_sequence = genome_sequence['sequence'] \n",
    "\n",
    "        # Load reference genes from cached GFF\n",
    "        ref = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None)\n",
    "        if (ref[2] == \"CDS\").sum() > 0:\n",
    "            ref_genes = ref[ref[2] == \"CDS\"][[3, 4]]\n",
    "        else:\n",
    "            ref_genes = ref[ref[2] == \"gene\"][[3, 4]]\n",
    "        ref_genes.columns = [\"start\", \"end\"]\n",
    "\n",
    "        genome_length = len(genome_sequence)\n",
    "        intergenic_results = {}\n",
    "\n",
    "        # -------------------\n",
    "        # Glimmer-style\n",
    "        # -------------------\n",
    "        likely_genes = [orf for orf in all_orfs if orf['length'] >= 200]\n",
    "        intergenic_seq_1, intergenic_coords_1 = extract_intergenic_regions(\n",
    "            genome_sequence, likely_genes, buffer=50, min_length=150\n",
    "        )\n",
    "        intergenic_results['glimmer'] = check_intergenic_purity_fast(intergenic_coords_1, ref_genes, genome_length, \"Glimmer\")\n",
    "\n",
    "        # -------------------\n",
    "        # RBS-filtered\n",
    "        # -------------------\n",
    "        intergenic_seq_2, intergenic_coords_2 = extract_non_orf_regions_conservative(\n",
    "            genome_sequence, all_orfs, min_rbs_threshold=3.0, min_length=150\n",
    "        )\n",
    "        intergenic_results['rbs_filtered'] = check_intergenic_purity_fast(intergenic_coords_2, ref_genes, genome_length, \"RBS-Filtered\")\n",
    "\n",
    "        # -------------------\n",
    "        # All non-ORF\n",
    "        # -------------------\n",
    "        intergenic_seq_3, intergenic_coords_3 = extract_all_non_orf_regions(\n",
    "            genome_sequence, all_orfs, min_length=150\n",
    "        )\n",
    "        intergenic_results['all_nonorf'] = check_intergenic_purity_fast(intergenic_coords_3, ref_genes, genome_length, \"All-NonORF\")\n",
    "\n",
    "        # -------------------\n",
    "        # Union of all three\n",
    "        # -------------------\n",
    "        all_union_coords = merge_intervals(intergenic_coords_1 + intergenic_coords_2 + intergenic_coords_3)\n",
    "        intergenic_results['union'] = check_intergenic_purity_fast(all_union_coords, ref_genes, genome_length, \"Union\")\n",
    "\n",
    "        # -------------------\n",
    "        # Flatten results\n",
    "        # -------------------\n",
    "        results = {\n",
    "            'genome_id': genome_id,\n",
    "            'total_orfs': len(all_orfs),\n",
    "            'likely_genes': len(likely_genes),\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'status': 'success'\n",
    "        }\n",
    "\n",
    "        for name, stats in intergenic_results.items():\n",
    "            for key, value in stats.items():\n",
    "                if key != 'method':\n",
    "                    results[f'{name}_{key}'] = value\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'genome_id': genome_id,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "def run_intergenic_test_precomputed_sequential(genomes, cached_candidates):\n",
    "    test_start = time.time()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SEQUENTIAL INTERGENIC PURITY TEST (PRECOMPUTED ORFs)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Started: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"Processing {len(genomes)} genome(s) sequentially\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    for id in genomes:\n",
    "        result = process_single_genome_intergenic(id, cached_candidates)\n",
    "        all_results.append(result)\n",
    "        status = \"✓\" if result.get('status') == 'success' else \"✗\"\n",
    "        print(f\"  {status} {id} ({result.get('processing_time',0):.1f}s) [{len(all_results)}/{len(genomes)}]\")\n",
    "\n",
    "    successful_results = [r for r in all_results if r.get('status') == 'success']\n",
    "    if not successful_results:\n",
    "        print(\"\\n✗ ERROR: All genomes failed to process!\")\n",
    "        return None, None\n",
    "\n",
    "    results_df = pd.DataFrame(successful_results)\n",
    "\n",
    "    summary_data = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        for strategy in ['glimmer', 'rbs_filtered', 'all_nonorf', 'union']:\n",
    "            summary_data.append({\n",
    "                'Strategy': strategy.replace('_',' ').title(),\n",
    "                'Regions': row[f'{strategy}_total_regions'],\n",
    "                'Pure BP': row[f'{strategy}_pure_bp'],\n",
    "                'BP Purity (%)': row[f'{strategy}_bp_purity'],\n",
    "                'Pct of Genome (%)': row[f'{strategy}_pct_of_genome']\n",
    "            })\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL INTERGENIC PURITY RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "\n",
    "    return summary_df, results_df\n",
    "\n",
    "def process_single_genome_precomputed(genome_id, cached_data, use_entropy=False):\n",
    "    \"\"\"\n",
    "    Process a single genome with precomputed ORFs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    genome_id : str\n",
    "        Genome identifier\n",
    "    cached_data : dict\n",
    "        Cached ORF data\n",
    "    use_entropy : bool, default=False\n",
    "        If True, includes entropy-based filtering strategies\n",
    "        If False, only uses strategies without entropy\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Use precomputed ORFs and GFF path\n",
    "        genome_data = cached_data.get(genome_id)\n",
    "        if genome_data is None:\n",
    "            raise ValueError(f\"No precomputed ORFs found for {genome_id}\")\n",
    "        \n",
    "        all_orfs = genome_data['orfs']\n",
    "        gff_path = get_gff_path(genome_id)\n",
    "\n",
    "        # Define strategies based on use_entropy flag\n",
    "        strategies = {}\n",
    "        sets_to_intersect = []\n",
    "        \n",
    "        # Always include glimmer_pure\n",
    "        glimmer_pure = select_training_glimmer(all_orfs, min_length=300, max_training_size=2000)\n",
    "        strategies['glimmer_pure'] = glimmer_pure\n",
    "        sets_to_intersect.append(\n",
    "            set((orf.get('genome_start', orf['start']), orf.get('genome_end', orf['end'])) \n",
    "                for orf in glimmer_pure)\n",
    "        )\n",
    "        \n",
    "        if use_entropy:\n",
    "            # Pre-calculate entropy for long ORFs (>=300)\n",
    "            long_orfs = [orf for orf in all_orfs if orf['length'] >= 300]\n",
    "            for orf in long_orfs:\n",
    "                calculate_amino_acid_entropy(orf)\n",
    "            \n",
    "            # Include entropy-based strategies\n",
    "            glimmer_3_0 = select_training_glimmer_3(\n",
    "                all_orfs, min_length=300, max_entropy=4.2, max_training_size=2000\n",
    "            )\n",
    "            enhanced_flexible = select_training_flexible(\n",
    "                all_orfs, target_size=2000, min_length=300, max_length=20000,\n",
    "                max_overlap_fraction=0.3, use_entropy_filter=True, max_entropy=4.2, prefer_atg=True\n",
    "            )\n",
    "            \n",
    "            strategies['glimmer_3_0'] = glimmer_3_0\n",
    "            strategies['enhanced_flexible'] = enhanced_flexible\n",
    "            \n",
    "            sets_to_intersect.append(\n",
    "                set((orf.get('genome_start', orf['start']), orf.get('genome_end', orf['end'])) \n",
    "                    for orf in glimmer_3_0)\n",
    "            )\n",
    "            sets_to_intersect.append(\n",
    "                set((orf.get('genome_start', orf['start']), orf.get('genome_end', orf['end'])) \n",
    "                    for orf in enhanced_flexible)\n",
    "            )\n",
    "        \n",
    "        # Always include enhanced_flexible_no_entropy\n",
    "        enhanced_flexible_no_entropy = select_training_flexible(\n",
    "            all_orfs, target_size=2000, min_length=300, max_length=20000,\n",
    "            max_overlap_fraction=0.3, use_entropy_filter=False, prefer_atg=True\n",
    "        )\n",
    "        strategies['enhanced_flexible_no_entropy'] = enhanced_flexible_no_entropy\n",
    "        sets_to_intersect.append(\n",
    "            set((orf.get('genome_start', orf['start']), orf.get('genome_end', orf['end'])) \n",
    "                for orf in enhanced_flexible_no_entropy)\n",
    "        )\n",
    "        \n",
    "        # Intersection of all strategies\n",
    "        intersection_coords = set.intersection(*sets_to_intersect)\n",
    "        intersection_all = [\n",
    "            orf for orf in all_orfs \n",
    "            if (orf.get('genome_start', orf['start']), orf.get('genome_end', orf['end'])) \n",
    "            in intersection_coords\n",
    "        ]\n",
    "        strategies['intersection'] = intersection_all\n",
    "        \n",
    "        # Load reference genes from cached GFF\n",
    "        ref = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None)\n",
    "        if (ref[2] == \"CDS\").sum() > 0:\n",
    "            ref_genes = ref[ref[2] == \"CDS\"][[3, 4]]\n",
    "        else:\n",
    "            ref_genes = ref[ref[2] == \"gene\"][[3, 4]]\n",
    "        ref_genes.columns = [\"start\", \"end\"]\n",
    "\n",
    "        # Calculate purity\n",
    "        results = {\n",
    "            'genome_id': genome_id,\n",
    "            'total_orfs': len(all_orfs),\n",
    "            'ref_genes': len(ref_genes),\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'status': 'success',\n",
    "            'use_entropy': use_entropy\n",
    "        }\n",
    "\n",
    "        for strategy_name, training_set in strategies.items():\n",
    "            training_coords = pd.DataFrame({\n",
    "                'start': [orf.get('genome_start', orf['start']) for orf in training_set],\n",
    "                'end': [orf.get('genome_end', orf['end']) for orf in training_set]\n",
    "            })\n",
    "            matches = pd.merge(training_coords, ref_genes, on=['start', 'end'])\n",
    "            true_genes = len(matches)\n",
    "            purity = (true_genes / len(training_set) * 100) if len(training_set) > 0 else 0\n",
    "\n",
    "            results[f'{strategy_name}_size'] = len(training_set)\n",
    "            results[f'{strategy_name}_true_genes'] = true_genes\n",
    "            results[f'{strategy_name}_purity'] = purity\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'genome_id': genome_id,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "def run_genome_test_precomputed_sequential(genomes, cached_data, use_entropy=False):\n",
    "    \"\"\"\n",
    "    Run sequential genome-level purity test with precomputed ORFs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    genomes : list\n",
    "        List of genome IDs to process\n",
    "    cached_data : dict\n",
    "        Cached ORF data\n",
    "    use_entropy : bool, default=False\n",
    "        If True, includes entropy-based filtering strategies\n",
    "        If False, only uses strategies without entropy\n",
    "    \"\"\"\n",
    "    test_start = time.time()\n",
    "    \n",
    "    entropy_status = \"WITH ENTROPY\" if use_entropy else \"WITHOUT ENTROPY\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"SEQUENTIAL GENOME-LEVEL PURITY TEST (PRECOMPUTED ORFs) - {entropy_status}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Started: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"Processing {len(genomes)} genome(s) sequentially\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    # Sequential execution\n",
    "    for genome_id in genomes:\n",
    "        result = process_single_genome_precomputed(genome_id, cached_data, use_entropy=use_entropy)\n",
    "        all_results.append(result)\n",
    "        status = \"✓\" if result.get('status') == 'success' else \"✗\"\n",
    "        print(f\"  {status} {genome_id} ({result.get('processing_time',0):.1f}s) [{len(all_results)}/{len(genomes)}]\")\n",
    "\n",
    "    total_time = time.time() - test_start\n",
    "\n",
    "    successful_results = [r for r in all_results if r.get('status') == 'success']\n",
    "    if not successful_results:\n",
    "        print(\"\\n✗ ERROR: All genomes failed to process!\")\n",
    "        return None, None\n",
    "\n",
    "    results_df = pd.DataFrame(successful_results)\n",
    "\n",
    "    # Determine which strategies to show\n",
    "    if use_entropy:\n",
    "        strategy_names = ['glimmer_pure', 'glimmer_3_0', 'enhanced_flexible', \n",
    "                         'enhanced_flexible_no_entropy', 'intersection']\n",
    "    else:\n",
    "        strategy_names = ['glimmer_pure', 'enhanced_flexible_no_entropy', 'intersection']\n",
    "\n",
    "    # Summary table\n",
    "    summary_data = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        for strategy in strategy_names:\n",
    "            if f'{strategy}_size' in row:\n",
    "                summary_data.append({\n",
    "                    'Strategy': strategy.replace('_', ' ').title(),\n",
    "                    'Training Size': row[f'{strategy}_size'],\n",
    "                    'True Genes': row[f'{strategy}_true_genes'],\n",
    "                    'Purity (%)': row[f'{strategy}_purity']\n",
    "                })\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL RESULTS TABLE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Average purity\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"AVERAGE PURITY BY STRATEGY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    avg_purity = summary_df.groupby('Strategy')['Purity (%)'].mean().sort_values(ascending=False)\n",
    "    for strategy, purity in avg_purity.items():\n",
    "        symbol = \"🏆\" if purity == avg_purity.max() else \"✓\"\n",
    "        print(f\"  {symbol} {strategy:<25} {purity:>6.2f}%\")\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    avg_processing_time = results_df['processing_time'].mean()\n",
    "    print(f\"  Average per-genome time: {avg_processing_time:.1f}s\")\n",
    "    print(f\"  Total wall-clock time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    print(f\"  Genomes processed: {len(successful_results)}/{len(genomes)}\")\n",
    "    print(f\"  Entropy filtering: {'Enabled' if use_entropy else 'Disabled'}\")\n",
    "\n",
    "    return summary_df, results_df\n",
    "'''\n",
    "# Test WITHOUT entropy\n",
    "summary_no_entropy, results_no_entropy = run_genome_test_precomputed_sequential(TEST_GENOMES, cached_data, use_entropy=False)\n",
    "# Test WITH entropy\n",
    "summary_with_entropy, results_with_entropy = run_genome_test_precomputed_sequential(TEST_GENOMES, cached_data, use_entropy=True)\n",
    "# Test INTERGENIC\n",
    "summary_df, results_df = run_intergenic_test_precomputed_sequential(TEST_GENOMES, cached_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9def4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(genome_id, cached_candidates, glimmer_max_size=2000, flexible_target_size=2000):\n",
    "    \"\"\"\n",
    "    Create a training set by intersecting Glimmer and Flexible (no-entropy) selections\n",
    "    for a given genome using precomputed ORFs from cached_candidates.\n",
    "\n",
    "    Args:\n",
    "        genome_id (str): Genome identifier\n",
    "        cached_candidates (dict): Precomputed ORFs for all genomes\n",
    "        glimmer_max_size (int): Maximum size for Glimmer training set\n",
    "        flexible_target_size (int): Target size for Flexible training set\n",
    "\n",
    "    Returns:\n",
    "        list: ORFs belonging to the intersection training set\n",
    "    \"\"\"\n",
    "    genome_data = cached_candidates.get(genome_id)\n",
    "    if genome_data is None:\n",
    "        raise ValueError(f\"No precomputed ORFs found for genome_id {genome_id}\")\n",
    "\n",
    "    all_orfs = genome_data['orfs']\n",
    "\n",
    "    # --- Step 1: Glimmer selection (pure, no entropy) ---\n",
    "    glimmer_set = select_training_glimmer(all_orfs, min_length=300, max_training_size=glimmer_max_size)\n",
    "\n",
    "    # --- Step 2: Flexible selection (no entropy) ---\n",
    "    flexible_set = select_training_flexible(\n",
    "        all_orfs, target_size=flexible_target_size, min_length=300, max_length=20000,\n",
    "        max_overlap_fraction=0.3, use_entropy_filter=False, prefer_atg=True\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Intersection based on coordinates ---\n",
    "    glimmer_coords = set((orf.get('genome_start', orf['start']),\n",
    "                          orf.get('genome_end', orf['end'])) for orf in glimmer_set)\n",
    "    flexible_coords = set((orf.get('genome_start', orf['start']),\n",
    "                           orf.get('genome_end', orf['end'])) for orf in flexible_set)\n",
    "\n",
    "    intersection_coords = glimmer_coords & flexible_coords\n",
    "\n",
    "    intersection_orfs = [orf for orf in all_orfs\n",
    "                         if (orf.get('genome_start', orf['start']),\n",
    "                             orf.get('genome_end', orf['end'])) in intersection_coords]\n",
    "\n",
    "    return intersection_orfs\n",
    "\n",
    "def create_intergenic_set(genome_id, cached_candidates, buffer=50, min_length=150, min_rbs_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Create intergenic regions by taking the union of multiple strategies:\n",
    "    - Glimmer-like ORFs (length >= 200)\n",
    "    - RBS-filtered non-ORF regions\n",
    "    - All non-ORF regions\n",
    "\n",
    "    Args:\n",
    "        genome_id (str): Genome identifier\n",
    "        cached_candidates (dict): Precomputed ORFs and genome sequences\n",
    "        buffer (int): Buffer around ORFs for Glimmer-like extraction\n",
    "        min_length (int): Minimum intergenic region length\n",
    "        min_rbs_threshold (float): Minimum RBS score for conservative extraction\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: intergenic regions, each with start, end, length, and sequence\n",
    "    \"\"\"\n",
    "    genome_data = cached_candidates.get(genome_id)\n",
    "    if genome_data is None:\n",
    "        raise ValueError(f\"No precomputed ORFs found for genome {genome_id}\")\n",
    "\n",
    "    genome_sequence = genome_data['sequence']\n",
    "    all_orfs = genome_data['orfs']\n",
    "\n",
    "    # Step 1: Identify likely coding regions (for exclusion)\n",
    "    likely_genes = [orf for orf in all_orfs if orf['length'] >= 200]\n",
    "\n",
    "    # Step 2: Extract intergenic regions from multiple strategies\n",
    "    _, intergenic_coords_1 = extract_intergenic_regions(\n",
    "        genome_sequence, likely_genes, buffer=buffer, min_length=min_length\n",
    "    )\n",
    "    _, intergenic_coords_2 = extract_non_orf_regions_conservative(\n",
    "        genome_sequence, all_orfs, min_rbs_threshold=min_rbs_threshold, min_length=min_length\n",
    "    )\n",
    "    _, intergenic_coords_3 = extract_all_non_orf_regions(\n",
    "        genome_sequence, all_orfs, min_length=min_length\n",
    "    )\n",
    "\n",
    "    # Step 3: Merge all coordinates into a single union\n",
    "    all_union_coords = merge_intervals(intergenic_coords_1 + intergenic_coords_2 + intergenic_coords_3)\n",
    "\n",
    "    # Step 4: Build dictionary objects similar to ORFs\n",
    "    intergenic_regions = []\n",
    "    for start, end in all_union_coords:\n",
    "        seq = genome_sequence[start-1:end]\n",
    "        intergenic_regions.append({\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'length': len(seq),\n",
    "            'sequence': seq,\n",
    "            'type': 'intergenic'\n",
    "        })\n",
    "\n",
    "    return intergenic_regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4673c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_codon_model(sequences):\n",
    "    \"\"\"\n",
    "    Build species-specific codon model from given sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): High-confidence sequences (either training or intergenic)\n",
    "        \n",
    "    Returns:\n",
    "        dict: codon -> frequency mapping\n",
    "    \"\"\"\n",
    "    codon_counts = Counter()\n",
    "    total_codons = 0\n",
    "\n",
    "    for seq in sequences:\n",
    "        sequence = seq['sequence']\n",
    "        for i in range(0, len(sequence) - 2, 3):\n",
    "            codon = sequence[i:i+3]\n",
    "            if len(codon) == 3 and 'N' not in codon:\n",
    "                codon_counts[codon] += 1\n",
    "                total_codons += 1\n",
    "\n",
    "    if total_codons == 0:\n",
    "        return {}\n",
    "\n",
    "    frequencies = {codon: count / total_codons for codon, count in codon_counts.items()}\n",
    "\n",
    "    return frequencies\n",
    "\n",
    "def prepare_models(training_orfs, intergenic_regions):\n",
    "    \"\"\"\n",
    "    Prepare both coding and background (noncoding) codon usage models.\n",
    "\n",
    "    Args:\n",
    "        training_orfs (list of dict): High-confidence ORFs with 'sequence' key\n",
    "        intergenic_regions (list of dict): Intergenic regions with 'sequence' key\n",
    "\n",
    "    Returns:\n",
    "        tuple: (codon_model, background_codon_model)\n",
    "            codon_model     -> dict: codon -> frequency\n",
    "            background_codon_model -> dict: codon -> frequency\n",
    "    \"\"\"\n",
    "    codon_model = build_codon_model(training_orfs)\n",
    "    background_codon_model = build_codon_model(intergenic_regions)\n",
    "\n",
    "    return codon_model, background_codon_model\n",
    "\n",
    "def score_codon_bias_ratio(orf_sequence, codon_model, background_codon_model):\n",
    "    \"\"\"\n",
    "    Score ORF by comparing coding vs background codon usage.\n",
    "    \n",
    "    Args:\n",
    "        orf_sequence (str): Sequence to score\n",
    "        codon_model (dict): Codon frequencies from genes\n",
    "        background_codon_model (dict): Codon frequencies from intergenic regions\n",
    "        \n",
    "    Returns:\n",
    "        float: Log ratio score (positive = more gene-like)\n",
    "    \"\"\"\n",
    "    if len(orf_sequence) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    coding_score = 0.0\n",
    "    background_score = 0.0\n",
    "    codon_count = 0\n",
    "    \n",
    "    for i in range(0, len(orf_sequence) - 2, 3):\n",
    "        codon = orf_sequence[i:i+3]\n",
    "        if len(codon) == 3 and 'N' not in codon:\n",
    "            # Get frequencies (with small pseudocount for unseen codons)\n",
    "            coding_freq = codon_model.get(codon, 0.0001)\n",
    "            background_freq = background_codon_model.get(codon, 0.0001)\n",
    "            \n",
    "            coding_score += math.log(coding_freq)\n",
    "            background_score += math.log(background_freq)\n",
    "            codon_count += 1\n",
    "    \n",
    "    if codon_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Return log likelihood ratio\n",
    "    return (coding_score - background_score) / codon_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0424b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attemp new imm with frame detection\n",
    "\n",
    "def build_interpolated_markov_model(training_sequences, max_order, min_observations=10):\n",
    "    \"\"\"\n",
    "    Build frame-aware IMM from training sequences.\n",
    "    Internally creates 3 position-specific models but API stays the same!\n",
    "    \n",
    "    Args:\n",
    "        training_sequences (list): List of DNA sequences (must start at codon position 0)\n",
    "        max_order (int): Maximum context length\n",
    "        min_observations (int): Minimum observations for reliable probability\n",
    "        \n",
    "    Returns:\n",
    "        list: [model_pos0, model_pos1, model_pos2] - 3 position-specific models\n",
    "              (internally frame-aware, but returned as list to maintain compatibility)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize 3 models, one for each codon position\n",
    "    position_models = [\n",
    "        defaultdict(lambda: defaultdict(int)),  # Position 0 (1st base of codon)\n",
    "        defaultdict(lambda: defaultdict(int)),  # Position 1 (2nd base of codon)\n",
    "        defaultdict(lambda: defaultdict(int))   # Position 2 (3rd base of codon)\n",
    "    ]\n",
    "    \n",
    "    # Build counts for each position\n",
    "    for sequence in training_sequences:\n",
    "        for i in range(len(sequence)):\n",
    "            nucleotide = sequence[i]\n",
    "            codon_position = i % 3  # Which position in the codon (0, 1, or 2)\n",
    "            \n",
    "            # Build context with different orders\n",
    "            for order in range(min(i + 1, max_order + 1)):\n",
    "                if order == 0:\n",
    "                    context = \"\"  # 0th order - no context\n",
    "                else:\n",
    "                    context = sequence[i-order:i]  # Previous 'order' nucleotides\n",
    "                \n",
    "                # Add to the model for this codon position\n",
    "                position_models[codon_position][context][nucleotide] += 1\n",
    "    \n",
    "    # Convert counts to probabilities for each position\n",
    "    position_probabilities = []\n",
    "    \n",
    "    for pos in range(3):\n",
    "        probabilities = {}\n",
    "        for context, nucleotide_counts in position_models[pos].items():\n",
    "            total_count = sum(nucleotide_counts.values())\n",
    "            \n",
    "            if total_count >= min_observations:  # Only if enough data\n",
    "                probabilities[context] = {}\n",
    "                for nucleotide, count in nucleotide_counts.items():\n",
    "                    probabilities[context][nucleotide] = count / total_count\n",
    "        \n",
    "        position_probabilities.append(probabilities)\n",
    "    \n",
    "    return position_probabilities\n",
    "\n",
    "def get_interpolated_probability(nucleotide, context, probabilities, fallback_prob=0.25):\n",
    "    \"\"\"\n",
    "    Get probability using longest reliable context.\n",
    "    Works with both single model (dict) or position-specific model (dict).\n",
    "    \n",
    "    Args:\n",
    "        nucleotide (str): Nucleotide to predict (A, T, G, C)\n",
    "        context (str): Context sequence (previous nucleotides)\n",
    "        probabilities (dict): Probability model for this position\n",
    "        fallback_prob (float): Default probability if no context found\n",
    "        \n",
    "    Returns:\n",
    "        float: Probability of nucleotide given context\n",
    "    \"\"\"\n",
    "    # Try longest context first, fall back to shorter contexts\n",
    "    for order in range(len(context), -1, -1):\n",
    "        current_context = context[-order:] if order > 0 else \"\"\n",
    "        \n",
    "        if current_context in probabilities:\n",
    "            if nucleotide in probabilities[current_context]:\n",
    "                return probabilities[current_context][nucleotide]\n",
    "    \n",
    "    # Ultimate fallback - uniform probability\n",
    "    return fallback_prob\n",
    "\n",
    "def score_imm_ratio(sequence, coding_imm, noncoding_imm, max_order):\n",
    "    \"\"\"\n",
    "    Score sequence using frame-aware IMM log-likelihood ratio.\n",
    "    \n",
    "    This function automatically detects if models are frame-aware (list of 3)\n",
    "    or frame-agnostic (single dict) and handles both!\n",
    "    \n",
    "    Args:\n",
    "        sequence (str): DNA sequence to score (should start at codon position 0)\n",
    "        coding_imm: Coding model (list of 3 dicts or single dict)\n",
    "        noncoding_imm: Noncoding model (list of 3 dicts or single dict)\n",
    "        max_order (int): Maximum context length\n",
    "        \n",
    "    Returns:\n",
    "        float: Log likelihood ratio (positive = more coding-like)\n",
    "    \"\"\"\n",
    "    if len(sequence) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    # Auto-detect if frame-aware (list of 3) or frame-agnostic (single dict)\n",
    "    is_frame_aware = isinstance(coding_imm, list) and len(coding_imm) == 3\n",
    "    \n",
    "    coding_log_prob = 0.0\n",
    "    noncoding_log_prob = 0.0\n",
    "    \n",
    "    # Pseudocount to prevent log(0)\n",
    "    EPSILON = 1e-10\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        nucleotide = sequence[i]\n",
    "        \n",
    "        # Get context (up to max_order previous nucleotides)\n",
    "        context_start = max(0, i - max_order)\n",
    "        context = sequence[context_start:i]\n",
    "        \n",
    "        if is_frame_aware:\n",
    "            # Frame-aware: use position-specific model\n",
    "            codon_position = i % 3  # Which position in codon (0, 1, or 2)\n",
    "            coding_prob = get_interpolated_probability(\n",
    "                nucleotide, context, coding_imm[codon_position]\n",
    "            )\n",
    "            noncoding_prob = get_interpolated_probability(\n",
    "                nucleotide, context, noncoding_imm[codon_position]\n",
    "            )\n",
    "        else:\n",
    "            # Frame-agnostic: use single model\n",
    "            coding_prob = get_interpolated_probability(nucleotide, context, coding_imm)\n",
    "            noncoding_prob = get_interpolated_probability(nucleotide, context, noncoding_imm)\n",
    "        \n",
    "        # Guard against log(0)\n",
    "        coding_prob = max(coding_prob, EPSILON)\n",
    "        noncoding_prob = max(noncoding_prob, EPSILON)\n",
    "        \n",
    "        # Add to log probabilities\n",
    "        coding_log_prob += math.log(coding_prob)\n",
    "        noncoding_log_prob += math.log(noncoding_prob)\n",
    "    \n",
    "    # Return log likelihood ratio normalized by sequence length\n",
    "    return (coding_log_prob - noncoding_log_prob) / len(sequence)\n",
    "\n",
    "def prepare_imm_models(training_set, intergenic_set, min_observations=10):\n",
    "    \"\"\"\n",
    "    Prepare frame-aware coding and noncoding IMM models.\n",
    "    Assumes both sets contain dicts with 'sequence' fields.\n",
    "    \"\"\"\n",
    "    # Extract sequences directly\n",
    "    training_seqs = [orf['sequence'] for orf in training_set]\n",
    "    intergenic_seqs = [orf['sequence'] for orf in intergenic_set]\n",
    "    \n",
    "    # Compute total sequence lengths\n",
    "    n_training = sum(len(seq) for seq in training_seqs)\n",
    "    n_intergenic = sum(len(seq) for seq in intergenic_seqs)\n",
    "    effective_n = min(n_training, n_intergenic)\n",
    "    \n",
    "    # Estimate max order based on available data\n",
    "    if effective_n < min_observations:\n",
    "        estimated_order = 0\n",
    "    else:\n",
    "        estimated_order = math.floor(math.log2(effective_n / min_observations) / 2)\n",
    "    \n",
    "    # Clip to reasonable bacterial range\n",
    "    estimated_order = min(estimated_order, 8)\n",
    "    estimated_order = max(estimated_order, 3)\n",
    "    \n",
    "    # Build frame-aware IMM models\n",
    "    coding_imm = build_interpolated_markov_model(training_seqs, estimated_order, min_observations)\n",
    "    noncoding_imm = build_interpolated_markov_model(intergenic_seqs, estimated_order, min_observations)\n",
    "    \n",
    "    return coding_imm, noncoding_imm, estimated_order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d1796b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_scoring_models(training_set, intergenic_set, min_observations=10):\n",
    "    \"\"\"\n",
    "    Build all scoring models needed for ORF evaluation.\n",
    "    \n",
    "    Args:\n",
    "        training_set (list): List of training ORFs or sequences\n",
    "        intergenic_set (list): List of intergenic sequences\n",
    "        min_observations (int): Minimum observations for IMM contexts\n",
    "        \n",
    "    Returns:\n",
    "        dict: All models needed for scoring:\n",
    "            - codon_model\n",
    "            - background_codon_model\n",
    "            - coding_imm\n",
    "            - noncoding_imm\n",
    "            - max_order\n",
    "    \"\"\"\n",
    "    print(\"Building scoring models...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"  Building codon usage models...\")\n",
    "    codon_model,background_codon_model = prepare_models(training_set,intergenic_set)\n",
    "\n",
    "    print(\"  Building IMM models...\")\n",
    "    coding_imm, noncoding_imm, max_order = prepare_imm_models(training_set, intergenic_set, min_observations=min_observations)\n",
    "    \n",
    "    print(f\"✓ All models built in {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'codon_model': codon_model,\n",
    "        'background_codon_model': background_codon_model,\n",
    "        'coding_imm': coding_imm,\n",
    "        'noncoding_imm': noncoding_imm,\n",
    "        'max_order': max_order\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a61bcc",
   "metadata": {},
   "source": [
    "ORF Scoring System\n",
    "\n",
    "## Goal\n",
    "Score all 176,315 ORF candidates using multiple evidence-based methods and rank them to identify real genes.\n",
    "\n",
    "## Scoring Methods\n",
    "1. **Codon Usage Bias** - Compare to coding vs non-coding patterns\n",
    "2. **Interpolated Markov Model** - Nucleotide context patterns \n",
    "3. **Ribosome Binding Site** - Translation initiation signals \n",
    "4. **ORF Length** \n",
    "5. **Start Codon** - Frequency-based weighting\n",
    "\n",
    "## Process\n",
    "1. Score each ORF with all methods (raw scores)\n",
    "2. Normalize scores to prevent range dominance\n",
    "3. Combine with weighted sum\n",
    "4. Rank all ORFs\n",
    "\n",
    "## Key Improvement\n",
    "**Normalization** prevents RBS (range -5 to 15) from dominating IMM (range -0.2 to 0.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ad01af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_orf_length(orf_length):\n",
    "    \"\"\"\n",
    "    Score ORF length using log scale.\n",
    "    \n",
    "    Args:\n",
    "        orf_length (int): Length of ORF in base pairs\n",
    "        \n",
    "    Returns:\n",
    "        float: Length score\n",
    "    \"\"\"\n",
    "    return math.log(max(orf_length, MIN_ORF_LENGTH) / LENGTH_REFERENCE_BP)\n",
    "\n",
    "def score_start_codon(start_codon):\n",
    "    \"\"\"\n",
    "    Score start codon based on frequency in bacterial genomes.\n",
    "    \"\"\"\n",
    "    return START_CODON_WEIGHTS.get(start_codon, 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9ccc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_all_orfs(all_orfs, models):\n",
    "    \"\"\"\n",
    "    Score all ORFs with all methods using pre-built models.\n",
    "    Adds all scores and RBS details to each ORF dict.\n",
    "    \n",
    "    Args:\n",
    "        all_orfs (list): List of ORF dictionaries\n",
    "        models (dict): Pre-built models from build_all_scoring_models()\n",
    "        \n",
    "    Returns:\n",
    "        list: ORFs with scores added\n",
    "    \"\"\"\n",
    "    print(f\"Scoring {len(all_orfs):,} ORFs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    codon_model = models['codon_model']\n",
    "    background_codon_model = models['background_codon_model']\n",
    "    coding_imm = models['coding_imm']\n",
    "    noncoding_imm = models['noncoding_imm']\n",
    "    max_order = models['max_order']\n",
    "    \n",
    "    for i, orf in enumerate(all_orfs):\n",
    "        if i % 25000 == 0 and i > 0:\n",
    "            print(f\"  {i:,}...\")\n",
    "        \n",
    "        # Score each component\n",
    "        orf['codon_score'] = score_codon_bias_ratio(\n",
    "            orf['sequence'], \n",
    "            codon_model, \n",
    "            background_codon_model\n",
    "        )\n",
    "        \n",
    "        orf['imm_score'] = score_imm_ratio(\n",
    "            orf['sequence'], \n",
    "            coding_imm, \n",
    "            noncoding_imm,\n",
    "            max_order\n",
    "        )\n",
    "        # Length score\n",
    "        orf['length_score'] = score_orf_length(orf['length'])\n",
    "        \n",
    "        # Start codon score\n",
    "        orf['start_score'] = score_start_codon(orf.get('start_codon', 'ATG'))\n",
    "    \n",
    "    print(f\"Done in {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    return all_orfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6f6c0",
   "metadata": {},
   "source": [
    "# normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7d8ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores_zscore(scores):\n",
    "    \"\"\"\n",
    "    Normalize scores using z-score (standard score).\n",
    "    \n",
    "    Formula: z = (x - mean) / std\n",
    "    \n",
    "    Args:\n",
    "        scores (list or array): Raw scores\n",
    "        \n",
    "    Returns:\n",
    "        numpy.array: Normalized scores (mean=0, std=1)\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    \n",
    "    if std == 0:  # All scores identical\n",
    "        return np.zeros_like(scores)\n",
    "    \n",
    "    return (scores - mean) / std\n",
    "\n",
    "def normalize_all_orf_scores(scored_orfs):\n",
    "    \"\"\"\n",
    "    Normalize all score components across all ORFs.\n",
    "    \n",
    "    Adds normalized versions to each ORF:\n",
    "    - codon_score_norm\n",
    "    - imm_score_norm\n",
    "    - rbs_score_norm\n",
    "    - length_score_norm\n",
    "    - start_score_norm\n",
    "    \n",
    "    Args:\n",
    "        scored_orfs (list): ORFs with raw scores\n",
    "        \n",
    "    Returns:\n",
    "        list: Same ORFs with normalized scores added\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nNormalizing ORFs...\")\n",
    "    \n",
    "    codon_scores = np.array([orf['codon_score'] for orf in scored_orfs])\n",
    "    imm_scores = np.array([orf['imm_score'] for orf in scored_orfs])\n",
    "    rbs_scores = np.array([orf['rbs_score'] for orf in scored_orfs])\n",
    "    length_scores = np.array([orf['length_score'] for orf in scored_orfs])\n",
    "    start_scores = np.array([orf['start_score'] for orf in scored_orfs])\n",
    "    \n",
    "    # Normalize each component\n",
    "    codon_norm = normalize_scores_zscore(codon_scores)\n",
    "    imm_norm = normalize_scores_zscore(imm_scores)\n",
    "    rbs_norm = normalize_scores_zscore(rbs_scores)\n",
    "    length_norm = normalize_scores_zscore(length_scores)\n",
    "    start_norm = normalize_scores_zscore(start_scores)\n",
    "    \n",
    "    # Add back to ORFs\n",
    "    for i, orf in enumerate(scored_orfs):\n",
    "        orf['codon_score_norm'] = codon_norm[i]\n",
    "        orf['imm_score_norm'] = imm_norm[i]\n",
    "        orf['rbs_score_norm'] = rbs_norm[i]\n",
    "        orf['length_score_norm'] = length_norm[i]\n",
    "        orf['start_score_norm'] = start_norm[i]\n",
    "    \n",
    "    components = [\n",
    "        ('Codon', codon_scores, codon_norm),\n",
    "        ('IMM', imm_scores, imm_norm),\n",
    "        ('RBS', rbs_scores, rbs_norm),\n",
    "        ('Length', length_scores, length_norm),\n",
    "        ('Start', start_scores, start_norm)\n",
    "    ]\n",
    "    \n",
    "    return scored_orfs\n",
    "\n",
    "def calculate_combined_score(orf, weights=None):\n",
    "    \"\"\"\n",
    "    Calculate weighted combined score from normalized components.\n",
    "    \n",
    "    Args:\n",
    "        orf (dict): ORF with normalized scores\n",
    "        weights (dict): Optional custom weights (uses global SCORE_WEIGHTS if None)\n",
    "        \n",
    "    Returns:\n",
    "        float: Combined score\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = SCORE_WEIGHTS  \n",
    "    \n",
    "    combined = (\n",
    "        orf['codon_score_norm'] * weights['codon'] +\n",
    "        orf['imm_score_norm'] * weights['imm'] +\n",
    "        orf['rbs_score_norm'] * weights['rbs'] +\n",
    "        orf['length_score_norm'] * weights['length'] +\n",
    "        orf['start_score_norm'] * weights['start']\n",
    "    )\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def add_combined_scores(scored_orfs, weights=None):\n",
    "    \"\"\"\n",
    "    Add combined score to all ORFs.\n",
    "    \n",
    "    Args:\n",
    "        scored_orfs (list): ORFs with normalized scores\n",
    "        weights (dict): Optional custom weights (uses global SCORE_WEIGHTS if None)\n",
    "        \n",
    "    Returns:\n",
    "        list: ORFs with combined_score added\n",
    "    \"\"\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = SCORE_WEIGHTS \n",
    "    \n",
    "    print(f\"\\nCalculating weighted scores...\")\n",
    "    \n",
    "    for orf in scored_orfs:\n",
    "        orf['combined_score'] = calculate_combined_score(orf, weights)\n",
    "    \n",
    "    combined_scores = np.array([orf['combined_score'] for orf in scored_orfs])\n",
    "     \n",
    "    return scored_orfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550a694",
   "metadata": {},
   "source": [
    "# filtering and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b160475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_rbs_implementation_fixed(all_orfs, reference_genes, genome_sequence=None):\n",
    "    \"\"\"\n",
    "    Fixed diagnostic that correctly displays RBS sequences from ORF data.\n",
    "    \n",
    "    Key fix: Instead of trying to re-extract sequences from genome,\n",
    "    use the 'rbs_sequence' field that was already calculated correctly.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RBS SCORE DIAGNOSTIC (FIXED)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Classify ORFs\n",
    "    print(\"\\n1. CLASSIFYING ORFs AS TP/FP...\")\n",
    "    tp_orfs = []\n",
    "    fp_orfs = []\n",
    "    \n",
    "    for orf in all_orfs:\n",
    "        orf_start = orf.get('genome_start', orf['start'])\n",
    "        orf_end = orf.get('genome_end', orf['end'])\n",
    "        is_match = (orf_start, orf_end) in reference_genes\n",
    "        \n",
    "        if is_match:\n",
    "            tp_orfs.append(orf)\n",
    "        else:\n",
    "            fp_orfs.append(orf)\n",
    "    \n",
    "    print(f\"   TP ORFs: {len(tp_orfs):,}\")\n",
    "    print(f\"   FP ORFs: {len(fp_orfs):,}\")\n",
    "    \n",
    "    # Step 2: Check RBS scores\n",
    "    print(\"\\n2. CHECKING RBS SCORE AVAILABILITY...\")\n",
    "    if not all_orfs or 'rbs_score' not in all_orfs[0]:\n",
    "        print(\"   ❌ ERROR: 'rbs_score' field not found!\")\n",
    "        return\n",
    "    print(\"   ✓ RBS scores found in ORFs\")\n",
    "    \n",
    "    # Step 3: Score distributions\n",
    "    print(\"\\n3. ANALYZING RBS SCORE DISTRIBUTIONS...\")\n",
    "    tp_scores = [orf['rbs_score'] for orf in tp_orfs if 'rbs_score' in orf]\n",
    "    fp_scores = [orf['rbs_score'] for orf in fp_orfs if 'rbs_score' in orf]\n",
    "    \n",
    "    if not tp_scores or not fp_scores:\n",
    "        print(\"   ❌ ERROR: No RBS scores available!\")\n",
    "        return\n",
    "    \n",
    "    tp_mean = np.mean(tp_scores)\n",
    "    fp_mean = np.mean(fp_scores)\n",
    "    tp_std = np.std(tp_scores)\n",
    "    fp_std = np.std(fp_scores)\n",
    "    tp_median = np.median(tp_scores)\n",
    "    fp_median = np.median(fp_scores)\n",
    "    \n",
    "    print(f\"   TP: mean={tp_mean:.4f}, std={tp_std:.4f}, median={tp_median:.4f}\")\n",
    "    print(f\"   FP: mean={fp_mean:.4f}, std={fp_std:.4f}, median={fp_median:.4f}\")\n",
    "    print(f\"   Mean difference: {tp_mean - fp_mean:.4f}\")\n",
    "    \n",
    "    pooled_std = np.sqrt((tp_std**2 + fp_std**2) / 2)\n",
    "    effect_size = abs(tp_mean - fp_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    print(f\"   Effect size (Cohen's d): {effect_size:.4f}\")\n",
    "    if effect_size < 0.5:\n",
    "        print(\"   ⚠️  Weak separation\")\n",
    "    elif effect_size < 0.8:\n",
    "        print(\"   ✓ Moderate separation\")\n",
    "    else:\n",
    "        print(\"   ✓✓ Good separation!\")\n",
    "    \n",
    "    # Step 4: FIXED - Analyze RBS sequences from ORF data\n",
    "    print(\"\\n4. ANALYZING RBS SEQUENCES FROM ORF DATA...\")\n",
    "    \n",
    "    tp_with_rbs = [o for o in tp_orfs if o.get('rbs_sequence')][:50]\n",
    "    fp_with_rbs = [o for o in fp_orfs if o.get('rbs_sequence')][:50]\n",
    "    \n",
    "    # Check for SD motifs in detected RBS sequences\n",
    "    sd_motifs = ['AGGAGG', 'GGAGG', 'AGGAG', 'GAGG', 'AGGA']\n",
    "    \n",
    "    print(f\"\\n   Shine-Dalgarno motif presence in detected RBS sequences:\")\n",
    "    print(f\"   (Analyzing {len(tp_with_rbs)} TP and {len(fp_with_rbs)} FP samples)\")\n",
    "    \n",
    "    for motif in sd_motifs:\n",
    "        tp_count = sum(1 for o in tp_with_rbs if motif in str(o.get('rbs_sequence', '')))\n",
    "        fp_count = sum(1 for o in fp_with_rbs if motif in str(o.get('rbs_sequence', '')))\n",
    "        tp_pct = tp_count / len(tp_with_rbs) * 100 if tp_with_rbs else 0\n",
    "        fp_pct = fp_count / len(fp_with_rbs) * 100 if fp_with_rbs else 0\n",
    "        print(f\"   {motif:8s}: TP={tp_pct:5.1f}%  FP={fp_pct:5.1f}%  (diff={tp_pct-fp_pct:+6.1f}%)\")\n",
    "    \n",
    "    print(\"\\n5. RBS SPACING DISTRIBUTION...\")\n",
    "    \n",
    "    tp_spacing = [o.get('rbs_spacing') for o in tp_orfs if o.get('rbs_spacing')]\n",
    "    fp_spacing = [o.get('rbs_spacing') for o in fp_orfs if o.get('rbs_spacing')]\n",
    "    \n",
    "    if tp_spacing:\n",
    "        from collections import Counter\n",
    "        tp_spacing_counts = Counter(tp_spacing)\n",
    "        fp_spacing_counts = Counter(fp_spacing)\n",
    "        \n",
    "        print(f\"\\n   Spacing distribution (nucleotides between SD and start codon):\")\n",
    "        print(f\"   {'Spacing':>8} {'TP Count':>10} {'TP %':>8} {'FP Count':>10} {'FP %':>8}\")\n",
    "        print(f\"   {'-'*8} {'-'*10} {'-'*8} {'-'*10} {'-'*8}\")\n",
    "        \n",
    "        all_spacings = sorted(set(tp_spacing_counts.keys()) | set(fp_spacing_counts.keys()))\n",
    "        for spacing in all_spacings[:15]:\n",
    "            tp_count = tp_spacing_counts.get(spacing, 0)\n",
    "            fp_count = fp_spacing_counts.get(spacing, 0)\n",
    "            tp_pct = tp_count / len(tp_spacing) * 100 if tp_spacing else 0\n",
    "            fp_pct = fp_count / len(fp_spacing) * 100 if fp_spacing else 0\n",
    "            print(f\"   {spacing:8d} {tp_count:10d} {tp_pct:7.1f}% {fp_count:10d} {fp_pct:7.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if effect_size >= 0.8:\n",
    "        print(\"\\n✓✓ RBS scoring is working well!\")\n",
    "        print(f\"   - Strong separation: Cohen's d = {effect_size:.3f}\")\n",
    "        print(f\"   - Mean difference: {tp_mean - fp_mean:.2f}\")\n",
    "    elif effect_size >= 0.5:\n",
    "        print(\"\\n✓ RBS scoring is working moderately well\")\n",
    "        print(f\"   - Moderate separation: Cohen's d = {effect_size:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n  RBS scoring needs improvement\")\n",
    "        print(f\"   - Weak separation: Cohen's d = {effect_size:.3f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "def diagnose_imm_implementation(\n",
    "    genome_id,\n",
    "    all_orfs,\n",
    "    cached_data,\n",
    "    models,\n",
    "    training_set=None,\n",
    "    intergenic_set=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostic for frame-aware IMM score problems.\n",
    "    \n",
    "    Args:\n",
    "        genome_id: Genome identifier\n",
    "        all_orfs: List of ORF dictionaries with 'imm_score'\n",
    "        cached_data: Cached data with genome info\n",
    "        models: Model dictionary from build_all_scoring_models()\n",
    "        training_set: Optional training sequences\n",
    "        intergenic_set: Optional intergenic sequences\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"FRAME-AWARE IMM DIAGNOSTIC\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load reference genes\n",
    "    print(\"\\n0. LOADING REFERENCE GENES...\")\n",
    "    try:\n",
    "        gff_path = get_gff_path(genome_id)\n",
    "        reference_genes = load_reference_genes_from_gff(gff_path)\n",
    "        print(f\"   Loaded {len(reference_genes):,} reference genes\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] {e}\")\n",
    "        return\n",
    "    \n",
    "    # Extract models\n",
    "    coding_imm = models['coding_imm']\n",
    "    noncoding_imm = models['noncoding_imm']\n",
    "    max_order = models['max_order']\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Classify ORFs as TP/FP\n",
    "    # ========================================================================\n",
    "    print(\"\\n1. CLASSIFYING ORFs AS TP/FP...\")\n",
    "    tp_orfs = []\n",
    "    fp_orfs = []\n",
    "    \n",
    "    for orf in all_orfs:\n",
    "        orf_start = orf.get('genome_start', orf['start'])\n",
    "        orf_end = orf.get('genome_end', orf['end'])\n",
    "        is_match = (orf_start, orf_end) in reference_genes\n",
    "        \n",
    "        if is_match:\n",
    "            tp_orfs.append(orf)\n",
    "        else:\n",
    "            fp_orfs.append(orf)\n",
    "    \n",
    "    print(f\"   TP ORFs: {len(tp_orfs):,}\")\n",
    "    print(f\"   FP ORFs: {len(fp_orfs):,}\")\n",
    "    \n",
    "    if len(tp_orfs) == 0:\n",
    "        print(\"   [CRITICAL] No true positives found!\")\n",
    "        return\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Check IMM scores\n",
    "    # ========================================================================\n",
    "    print(\"\\n2. CHECKING IMM SCORES...\")\n",
    "    if not all_orfs or 'imm_score' not in all_orfs[0]:\n",
    "        print(\"   [ERROR] No IMM scores found!\")\n",
    "        return\n",
    "    print(\"   IMM scores present\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Analyze score distributions\n",
    "    # ========================================================================\n",
    "    print(\"\\n3. ANALYZING IMM SCORE DISTRIBUTIONS...\")\n",
    "    tp_scores = [orf['imm_score'] for orf in tp_orfs]\n",
    "    fp_scores = [orf['imm_score'] for orf in fp_orfs]\n",
    "    \n",
    "    tp_mean = np.mean(tp_scores)\n",
    "    fp_mean = np.mean(fp_scores)\n",
    "    tp_std = np.std(tp_scores)\n",
    "    fp_std = np.std(fp_scores)\n",
    "    tp_median = np.median(tp_scores)\n",
    "    fp_median = np.median(fp_scores)\n",
    "    \n",
    "    print(f\"   TP: mean={tp_mean:.6f}, std={tp_std:.6f}, median={tp_median:.6f}\")\n",
    "    print(f\"   FP: mean={fp_mean:.6f}, std={fp_std:.6f}, median={fp_median:.6f}\")\n",
    "    print(f\"   Mean difference: {tp_mean - fp_mean:.6f}\")\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((tp_std**2 + fp_std**2) / 2)\n",
    "    effect_size = abs(tp_mean - fp_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    print(f\"   Effect size (Cohen's d): {effect_size:.4f}\")\n",
    "    if effect_size < 0.2:\n",
    "        print(\"   [WARNING] Very weak separation\")\n",
    "    elif effect_size < 0.5:\n",
    "        print(\"   [WARNING] Weak separation\")\n",
    "    elif effect_size < 0.8:\n",
    "        print(\"   Moderate separation\")\n",
    "    else:\n",
    "        print(\"   Strong separation\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Check frame-aware model structure\n",
    "    # ========================================================================\n",
    "    print(\"\\n4. CHECKING FRAME-AWARE MODEL STRUCTURE...\")\n",
    "    print(f\"   Max order: {max_order}\")\n",
    "    \n",
    "    # Verify frame-aware structure (must be list of 3)\n",
    "    if not (isinstance(coding_imm, list) and len(coding_imm) == 3):\n",
    "        print(\"   [ERROR] Not a frame-aware IMM! Expected list of 3 models.\")\n",
    "        return\n",
    "    \n",
    "    print(\"   Frame-aware IMM confirmed (3 position-specific models)\")\n",
    "    \n",
    "    for pos in range(3):\n",
    "        print(f\"   Position {pos}: coding={len(coding_imm[pos])} contexts, \"\n",
    "              f\"noncoding={len(noncoding_imm[pos])} contexts\")\n",
    "    \n",
    "    # Sample from position 0 model\n",
    "    print(f\"\\n   Sampling contexts from Position 0:\")\n",
    "    sample_contexts = list(coding_imm[0].keys())[:5]\n",
    "    \n",
    "    models_differ = False\n",
    "    for context in sample_contexts:\n",
    "        if context in coding_imm[0] and context in noncoding_imm[0]:\n",
    "            coding_probs = coding_imm[0][context]\n",
    "            noncoding_probs = noncoding_imm[0][context]\n",
    "            \n",
    "            for nuc in ['A', 'T', 'G', 'C']:\n",
    "                c_prob = coding_probs.get(nuc, 0)\n",
    "                nc_prob = noncoding_probs.get(nuc, 0)\n",
    "                diff = abs(c_prob - nc_prob)\n",
    "                \n",
    "                if diff > 0.01:\n",
    "                    models_differ = True\n",
    "                \n",
    "                if diff > 0.05:\n",
    "                    print(f\"   Context '{context}'+{nuc}: \"\n",
    "                          f\"coding={c_prob:.3f}, noncoding={nc_prob:.3f} (diff={diff:.3f})\")\n",
    "    \n",
    "    if not models_differ:\n",
    "        print(\"   [CRITICAL] Models nearly identical\")\n",
    "    else:\n",
    "        print(\"   Models show differences\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Check training data quality\n",
    "    # ========================================================================\n",
    "    if training_set and intergenic_set:\n",
    "        print(\"\\n5. CHECKING TRAINING DATA...\")\n",
    "        \n",
    "        train_seqs = [s['sequence'] for s in training_set]\n",
    "        inter_seqs = [s['sequence'] for s in intergenic_set]\n",
    "        \n",
    "        total_coding_bp = sum(len(s) for s in train_seqs)\n",
    "        total_noncoding_bp = sum(len(s) for s in inter_seqs)\n",
    "        \n",
    "        print(f\"   Coding: {len(train_seqs)} sequences, {total_coding_bp:,} bp\")\n",
    "        print(f\"   Noncoding: {len(inter_seqs)} sequences, {total_noncoding_bp:,} bp\")\n",
    "        \n",
    "        # GC content\n",
    "        coding_gc = sum(s.count('G') + s.count('C') for s in train_seqs) / total_coding_bp\n",
    "        noncoding_gc = sum(s.count('G') + s.count('C') for s in inter_seqs) / total_noncoding_bp\n",
    "        \n",
    "        print(f\"   Coding GC%: {coding_gc*100:.2f}%\")\n",
    "        print(f\"   Noncoding GC%: {noncoding_gc*100:.2f}%\")\n",
    "        print(f\"   GC difference: {abs(coding_gc - noncoding_gc)*100:.2f}%\")\n",
    "        \n",
    "        if abs(coding_gc - noncoding_gc) < 0.02:\n",
    "            print(\"   [WARNING] Very similar GC content\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Diagnostic plots\n",
    "    # ========================================================================\n",
    "    print(\"\\n6. GENERATING PLOTS...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Score distributions\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(fp_scores, bins=50, alpha=0.6, color='red', \n",
    "             label=f'FP (n={len(fp_scores):,})', density=True)\n",
    "    ax1.hist(tp_scores, bins=50, alpha=0.6, color='green', \n",
    "             label=f'TP (n={len(tp_scores):,})', density=True)\n",
    "    ax1.axvline(tp_mean, color='darkgreen', linestyle='--', linewidth=2, \n",
    "                label=f'TP mean: {tp_mean:.4f}')\n",
    "    ax1.axvline(fp_mean, color='darkred', linestyle='--', linewidth=2, \n",
    "                label=f'FP mean: {fp_mean:.4f}')\n",
    "    ax1.set_xlabel('IMM Score')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Frame-Aware IMM: TP vs FP Distribution')\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Score vs Length\n",
    "    ax2 = axes[1]\n",
    "    tp_lengths = [len(orf.get('sequence', '')) for orf in tp_orfs[:1000]]\n",
    "    fp_lengths = [len(orf.get('sequence', '')) for orf in fp_orfs[:1000]]\n",
    "    ax2.scatter(tp_lengths, tp_scores[:1000], alpha=0.3, s=10, color='green', label='TP')\n",
    "    ax2.scatter(fp_lengths, fp_scores[:1000], alpha=0.3, s=10, color='red', label='FP')\n",
    "    ax2.set_xlabel('ORF Length (bp)')\n",
    "    ax2.set_ylabel('IMM Score')\n",
    "    ax2.set_title('IMM Score vs ORF Length')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 7: Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIAGNOSTIC SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    if effect_size < 0.2:\n",
    "        issues.append(\"CRITICAL: Very weak TP/FP separation\")\n",
    "    \n",
    "    if not models_differ:\n",
    "        issues.append(\"CRITICAL: Models nearly identical\")\n",
    "    \n",
    "    total_contexts = sum(len(coding_imm[i]) for i in range(3))\n",
    "    if total_contexts < 30:\n",
    "        issues.append(\"WARNING: Insufficient contexts (<30)\")\n",
    "    \n",
    "    if max_order < 3:\n",
    "        issues.append(\"WARNING: Low order model (<3)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n[ISSUES FOUND]\")\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "        \n",
    "        print(\"\\n[RECOMMENDATIONS]\")\n",
    "        print(\"   1. Verify training sequences are true coding regions (CDS)\")\n",
    "        print(\"   2. Verify intergenic sequences are non-coding\")\n",
    "        print(\"   3. Increase training data (aim for >100kb per class)\")\n",
    "        print(\"   4. Check sequences are in correct reading frame\")\n",
    "    else:\n",
    "        print(\"\\n[STATUS] Frame-aware IMM working well\")\n",
    "        print(f\"   Effect size: {effect_size:.3f}\")\n",
    "        print(f\"   Total contexts: {total_contexts}\")\n",
    "    \n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae1e59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_candidates(all_orfs, codon_threshold=0, imm_threshold=0, length_threshold=0, combined_threshold=0):\n",
    "    \"\"\"\n",
    "    Removes ORFs if:\n",
    "    - ALL THREE scores (codon, imm, length) are below their thresholds, OR\n",
    "    - combined_score is below its threshold\n",
    "\n",
    "    \"\"\"\n",
    "    filtered_orfs = []\n",
    "    \n",
    "    for orf in all_orfs:\n",
    "        length_score = orf.get('length_score', 0)\n",
    "        codon_score = orf.get('codon_score', 0)\n",
    "        imm_score = orf.get('imm_score', 0)\n",
    "        combined_score = orf.get('combined_score', 0)\n",
    "        \n",
    "        # Remove if ALL THREE are below thresholds OR if combined_score is below threshold\n",
    "        all_three_below = (length_score < length_threshold and \n",
    "                          codon_score < codon_threshold and \n",
    "                          imm_score < imm_threshold)\n",
    "        combined_below = combined_score < combined_threshold\n",
    "        \n",
    "        if all_three_below or combined_below:\n",
    "            continue\n",
    "        else:\n",
    "            filtered_orfs.append(orf)\n",
    "    \n",
    "    return filtered_orfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4837e",
   "metadata": {},
   "source": [
    "## Known Limitation: Start Codon Selection\n",
    "\n",
    "Example: ORF at 2,420,617-2,421,960 (1344 bp)\n",
    "- BLAST: 100% match to 215 AA protein (645 bp)\n",
    "- Issue: Algorithm selected upstream ATG\n",
    "- Real start: ~2,421,317 (700 bp downstream)\n",
    "\n",
    "Challenge: Multiple ATGs in same frame\n",
    "- ORF detection finds longest possible ORF???\n",
    "- Need better start codon discrimination???\n",
    "- RBS positioning is key???\n",
    "\n",
    "Possible improvements:\n",
    "- Weight RBS more heavily for start selection\n",
    "- Penalize unusually long ORFs without strong evidence\n",
    "- Check alternative internal starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b51ceb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_nested_orfs(all_orfs):\n",
    "    \"\"\"\n",
    "    Group ORFs by stop codon, sort by start position within each group.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {(strand, end_position): [list of ORFs sorted by start]}\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    \n",
    "    for orf in all_orfs:\n",
    "        key = (orf['strand'], orf['end'])\n",
    "        groups[key].append(orf)\n",
    "    \n",
    "    for key in groups:\n",
    "        groups[key].sort(key=lambda x: x['start'])\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def select_best_starts(nested_groups, weights=None):\n",
    "    \"\"\"\n",
    "    For each stop codon, select the best start position using multi-factor scoring.\n",
    "    Uses START_SELECTION_WEIGHTS by default to prioritize biological signals\n",
    "    over length when choosing between nested ORFs.\n",
    "    \n",
    "    Args:\n",
    "        nested_groups (dict): {(strand, end): [list of ORFs with same stop]}\n",
    "        weights (dict): Optional custom weights for start selection\n",
    "        \n",
    "    Returns:\n",
    "        list: Selected ORFs (one per stop position)\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = START_SELECTION_WEIGHTS\n",
    "    \n",
    "    print(f\"\\nSelecting best start for {len(nested_groups):,}\")\n",
    "    \n",
    "    selected_orfs = []\n",
    "    single_option = 0\n",
    "    multiple_options = 0\n",
    "    \n",
    "    selection_reasons = {\n",
    "        'rbs_winner': 0,\n",
    "        'imm_winner': 0,\n",
    "        'codon_winner': 0,\n",
    "        'length_winner': 0\n",
    "    }\n",
    "    \n",
    "    for (strand, end), orfs in nested_groups.items():\n",
    "        if len(orfs) == 1:\n",
    "            selected_orfs.append(orfs[0])\n",
    "            single_option += 1\n",
    "        else:\n",
    "            # RECALCULATE score using start-selaection weights\n",
    "            for orf in orfs:\n",
    "                orf['start_selection_score'] = (\n",
    "                    orf['codon_score_norm'] * weights['codon'] +\n",
    "                    orf['imm_score_norm'] * weights['imm'] +\n",
    "                    orf['rbs_score_norm'] * weights['rbs'] +\n",
    "                    orf['length_score_norm'] * weights['length'] +\n",
    "                    orf['start_score_norm'] * weights['start']\n",
    "                )\n",
    "            \n",
    "            # Select based on the NEW score\n",
    "            best_orf = max(orfs, key=lambda x: x['start_selection_score'])\n",
    "            selected_orfs.append(best_orf)\n",
    "            multiple_options += 1\n",
    "            \n",
    "            # Track selection reasons\n",
    "            components = ['rbs_score_norm', 'imm_score_norm', 'codon_score_norm', 'length_score_norm']\n",
    "            component_names = ['rbs_winner', 'imm_winner', 'codon_winner', 'length_winner']\n",
    "            \n",
    "            best_component_value = -999\n",
    "            best_component = None\n",
    "            for comp, name in zip(components, component_names):\n",
    "                if best_orf[comp] > best_component_value:\n",
    "                    best_component_value = best_orf[comp]\n",
    "                    best_component = name\n",
    "            \n",
    "            if best_component:\n",
    "                selection_reasons[best_component] += 1\n",
    "    \n",
    "    return selected_orfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a8dafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_groups_correct_vs_selected(nested_groups, genome_id, weights=None):\n",
    "    \"\"\"\n",
    "    Compare, for all reference-containing groups, the real gene vs the algorithm's selected ORF.\n",
    "    Performs component-wise comparisons and reports:\n",
    "      - % correct selections\n",
    "      - Which ORF wins more scoring components per group\n",
    "      - Optional per-score preference summary\n",
    "    \n",
    "    Args:\n",
    "        nested_groups: Dictionary of grouped ORFs\n",
    "        genome_id: Genome accession ID (e.g., \"NC_000913.3\")\n",
    "        weights: Optional dict of score weights\n",
    "    \n",
    "    Returns:\n",
    "        list: Analyzed groups with comparison results\n",
    "    \"\"\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = START_SELECTION_WEIGHTS\n",
    "    \n",
    "    # Load reference coordinates using proper function\n",
    "    gff_path = get_gff_path(genome_id)\n",
    "    ref = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None)\n",
    "    ref_genes = ref[ref[2] == \"CDS\"][[3, 4]]\n",
    "    ref_genes.columns = [\"start\", \"end\"]\n",
    "    ref_genes = ref_genes.drop_duplicates()\n",
    "    ref_set = set(zip(ref_genes['start'], ref_genes['end']))\n",
    "    \n",
    "    analyzed_groups = []\n",
    "\n",
    "    # Helper for weighted selection\n",
    "    def weighted_score(orf):\n",
    "        return (\n",
    "            orf['codon_score_norm'] * weights.get('codon', 0) +\n",
    "            orf['imm_score_norm'] * weights.get('imm', 0) +\n",
    "            orf['rbs_score_norm'] * weights.get('rbs', 0) +\n",
    "            orf['length_score_norm'] * weights.get('length', 0) +\n",
    "            orf['start_score_norm'] * weights.get('start', 0)\n",
    "        )\n",
    "\n",
    "    # Iterate over groups\n",
    "    for orfs_at_stop in nested_groups.values():\n",
    "        if not orfs_at_stop:\n",
    "            continue\n",
    "\n",
    "        # Find reference orf\n",
    "        correct_orf = None\n",
    "        for orf in orfs_at_stop:\n",
    "            coord = (orf.get('genome_start', orf['start']), orf.get('genome_end', orf['end']))\n",
    "            if coord in ref_set:\n",
    "                correct_orf = orf\n",
    "                break\n",
    "        if correct_orf is None:\n",
    "            continue\n",
    "\n",
    "        # Skip singletons\n",
    "        if len(orfs_at_stop) == 1:\n",
    "            continue\n",
    "\n",
    "        selected_orf = max(orfs_at_stop, key=weighted_score)\n",
    "\n",
    "        # Component-wise comparison\n",
    "        score_keys = ['codon_score_norm', 'imm_score_norm', 'rbs_score_norm', \n",
    "                      'length_score_norm', 'start_score_norm']\n",
    "        correct_points = 0\n",
    "        selected_points = 0\n",
    "        per_component = {}\n",
    "\n",
    "        for key in score_keys:\n",
    "            c_val = correct_orf.get(key, 0)\n",
    "            s_val = selected_orf.get(key, 0)\n",
    "            if c_val > s_val:\n",
    "                correct_points += 1\n",
    "                per_component[key] = \"correct\"\n",
    "            elif s_val > c_val:\n",
    "                selected_points += 1\n",
    "                per_component[key] = \"selected\"\n",
    "            else:\n",
    "                per_component[key] = \"tie\"\n",
    "\n",
    "        # Exact match test\n",
    "        selected_coord = (selected_orf.get('genome_start', selected_orf['start']),\n",
    "                         selected_orf.get('genome_end', selected_orf['end']))\n",
    "        correct_coord = (correct_orf.get('genome_start', correct_orf['start']),\n",
    "                        correct_orf.get('genome_end', correct_orf['end']))\n",
    "        \n",
    "        analyzed_groups.append({\n",
    "            'correct_orf': correct_orf,\n",
    "            'selected_orf': selected_orf,\n",
    "            'correct_points': correct_points,\n",
    "            'selected_points': selected_points,\n",
    "            'is_correct': selected_coord == correct_coord,\n",
    "            'group_size': len(orfs_at_stop),\n",
    "            **per_component\n",
    "        })\n",
    "\n",
    "    # Reporting\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CORRECT vs SELECTED ANALYSIS: {genome_id}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total groups with reference genes: {len(analyzed_groups)}\")\n",
    "    print(f\"Selection weights: {weights}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    correct_selections = [g for g in analyzed_groups if g['is_correct']]\n",
    "    wrong_selections = [g for g in analyzed_groups if not g['is_correct']]\n",
    "\n",
    "    print(f\"\\nSelection results:\")\n",
    "    print(f\"  Correct selections: {len(correct_selections)} ({len(correct_selections)/len(analyzed_groups)*100:.1f}%)\")\n",
    "    print(f\"  Wrong selections:   {len(wrong_selections)} ({len(wrong_selections)/len(analyzed_groups)*100:.1f}%)\")\n",
    "\n",
    "    # Point comparison\n",
    "    correct_won_more = sum(1 for g in analyzed_groups if g['correct_points'] > g['selected_points'])\n",
    "    selected_won_more = sum(1 for g in analyzed_groups if g['selected_points'] > g['correct_points'])\n",
    "    tied = len(analyzed_groups) - correct_won_more - selected_won_more\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POINT COMPARISON (Correct vs Selected)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Correct gene won more components:   {correct_won_more:>4} ({correct_won_more/len(analyzed_groups)*100:>5.1f}%)\")\n",
    "    print(f\"  Selected gene won more components:  {selected_won_more:>4} ({selected_won_more/len(analyzed_groups)*100:>5.1f}%)\")\n",
    "    print(f\"  Tied:                               {tied:>4} ({tied/len(analyzed_groups)*100:>5.1f}%)\")\n",
    "    print(f\"\\nNote: {len(correct_selections)} groups where correct=selected contribute to 'Tied'\")\n",
    "\n",
    "    # Per-score summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCORE COMPONENT PREFERENCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    for key in ['codon_score_norm', 'imm_score_norm', 'rbs_score_norm', \n",
    "                'length_score_norm', 'start_score_norm']:\n",
    "        correct_favored = sum(1 for g in analyzed_groups if g[key] == 'correct')\n",
    "        selected_favored = sum(1 for g in analyzed_groups if g[key] == 'selected')\n",
    "        tied_comp = sum(1 for g in analyzed_groups if g[key] == 'tie')\n",
    "        print(f\"{key:20s} -> correct: {correct_favored:5}  selected: {selected_favored:5}  tied: {tied_comp:5}\")\n",
    "\n",
    "    return analyzed_groups\n",
    "\n",
    "def comprehensive_fn_analysis(nested_groups, genome_id, weights=None):\n",
    "    \"\"\"\n",
    "    Complete analysis of false negatives from start selection.\n",
    "    Shows raw scores, weighted contributions, and point-based comparison.\n",
    "    \n",
    "    Args:\n",
    "        nested_groups: Dictionary of grouped ORFs\n",
    "        genome_id: Genome accession ID (e.g., \"NC_000913.3\")\n",
    "        weights: Optional dict of score weights\n",
    "    \n",
    "    Returns:\n",
    "        dict: FN statistics and analysis results\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = weights = START_SELECTION_WEIGHTS\n",
    "        \n",
    "    # Load reference\n",
    "    gff_path = get_gff_path(genome_id)\n",
    "    ref = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None)\n",
    "    ref_genes = ref[ref[2] == \"CDS\"][[3, 4]]\n",
    "    ref_genes.columns = [\"start\", \"end\"]\n",
    "    ref_genes = ref_genes.drop_duplicates()\n",
    "    ref_set = set(zip(ref_genes['start'], ref_genes['end']))\n",
    "    \n",
    "    lost_genes = []\n",
    "    \n",
    "    def weighted_score(orf):\n",
    "        return (\n",
    "            orf['codon_score_norm'] * weights.get('codon', 0) +\n",
    "            orf['imm_score_norm'] * weights.get('imm', 0) +\n",
    "            orf['rbs_score_norm'] * weights.get('rbs', 0) +\n",
    "            orf['length_score_norm'] * weights.get('length', 0) +\n",
    "            orf['start_score_norm'] * weights.get('start', 0)\n",
    "        )\n",
    "    \n",
    "    for orfs_at_stop in nested_groups.values():\n",
    "        if len(orfs_at_stop) == 1:\n",
    "            continue\n",
    "        \n",
    "        correct_orf = None\n",
    "        for orf in orfs_at_stop:\n",
    "            coord = (orf.get('genome_start', orf['start']), \n",
    "                    orf.get('genome_end', orf['end']))\n",
    "            if coord in ref_set:\n",
    "                correct_orf = orf\n",
    "                break\n",
    "        \n",
    "        if correct_orf is None:\n",
    "            continue\n",
    "        \n",
    "        selected_orf = max(orfs_at_stop, key=weighted_score)\n",
    "        \n",
    "        selected_coord = (selected_orf.get('genome_start', selected_orf['start']),\n",
    "                         selected_orf.get('genome_end', selected_orf['end']))\n",
    "        correct_coord = (correct_orf.get('genome_start', correct_orf['start']),\n",
    "                        correct_orf.get('genome_end', correct_orf['end']))\n",
    "        \n",
    "        if selected_coord != correct_coord:\n",
    "            # Calculate points\n",
    "            score_keys = ['codon_score_norm', 'imm_score_norm', 'rbs_score_norm',\n",
    "                         'length_score_norm', 'start_score_norm']\n",
    "            correct_points = 0\n",
    "            selected_points = 0\n",
    "            \n",
    "            for key in score_keys:\n",
    "                if correct_orf[key] > selected_orf[key]:\n",
    "                    correct_points += 1\n",
    "                elif selected_orf[key] > correct_orf[key]:\n",
    "                    selected_points += 1\n",
    "            \n",
    "            lost_genes.append({\n",
    "                'correct': correct_orf,\n",
    "                'selected': selected_orf,\n",
    "                'correct_points': correct_points,\n",
    "                'selected_points': selected_points\n",
    "            })\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPREHENSIVE FALSE NEGATIVE ANALYSIS: {genome_id}\")\n",
    "    print(f\"Total genes lost: {len(lost_genes)}\")\n",
    "    print(f\"Selection weights: {weights}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not lost_genes:\n",
    "        print(\"\\nNo false negatives with these weights\")\n",
    "        return {'fn_count': 0}\n",
    "    \n",
    "    # Calculate all statistics\n",
    "    imm_raw = []\n",
    "    codon_raw = []\n",
    "    rbs_raw = []\n",
    "    length_raw = []\n",
    "    start_raw = []\n",
    "    \n",
    "    imm_weighted = []\n",
    "    codon_weighted = []\n",
    "    rbs_weighted = []\n",
    "    length_weighted = []\n",
    "    start_weighted = []\n",
    "    \n",
    "    for case in lost_genes:\n",
    "        sel = case['selected']\n",
    "        cor = case['correct']\n",
    "        \n",
    "        # Raw differences\n",
    "        imm_raw.append(sel['imm_score_norm'] - cor['imm_score_norm'])\n",
    "        codon_raw.append(sel['codon_score_norm'] - cor['codon_score_norm'])\n",
    "        rbs_raw.append(sel['rbs_score_norm'] - cor['rbs_score_norm'])\n",
    "        length_raw.append(sel['length_score_norm'] - cor['length_score_norm'])\n",
    "        start_raw.append(sel['start_score_norm'] - cor['start_score_norm'])\n",
    "        \n",
    "        # Weighted differences\n",
    "        imm_weighted.append((sel['imm_score_norm'] - cor['imm_score_norm']) * weights.get('imm', 0))\n",
    "        codon_weighted.append((sel['codon_score_norm'] - cor['codon_score_norm']) * weights.get('codon', 0))\n",
    "        rbs_weighted.append((sel['rbs_score_norm'] - cor['rbs_score_norm']) * weights.get('rbs', 0))\n",
    "        length_weighted.append((sel['length_score_norm'] - cor['length_score_norm']) * weights.get('length', 0))\n",
    "        start_weighted.append((sel['start_score_norm'] - cor['start_score_norm']) * weights.get('start', 0))\n",
    "    \n",
    "    # Build summary table\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(f\"{'Component':<10} | {'Weight':<6} | {'Raw Diff':<20} | {'Weighted Contrib':<20} | {'Favored Correct':<15} | {'Favored Wrong':<15}\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    components = [\n",
    "        ('Codon', codon_raw, codon_weighted, weights.get('codon', 0)),\n",
    "        ('IMM', imm_raw, imm_weighted, weights.get('imm', 0)),\n",
    "        ('RBS', rbs_raw, rbs_weighted, weights.get('rbs', 0)),\n",
    "        ('Length', length_raw, length_weighted, weights.get('length', 0)),\n",
    "        ('Start', start_raw, start_weighted, weights.get('start', 0))\n",
    "    ]\n",
    "    \n",
    "    for name, raw, weighted, weight in components:\n",
    "        favored_correct = sum(1 for r in raw if r < 0)\n",
    "        favored_wrong = sum(1 for r in raw if r > 0)\n",
    "        \n",
    "        raw_mean = np.mean(raw)\n",
    "        weighted_mean = np.mean(weighted) if weight != 0 else 0\n",
    "        \n",
    "        if weight == 0:\n",
    "            print(f\"{name:<10} | {weight:>6.1f} | {raw_mean:>8.3f} (mean)    | {'DISABLED':>20} | {favored_correct:>4} ({favored_correct/len(lost_genes)*100:>5.1f}%) | {favored_wrong:>4} ({favored_wrong/len(lost_genes)*100:>5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{name:<10} | {weight:>6.1f} | {raw_mean:>8.3f} (mean)    | {weighted_mean:>8.3f} (mean)    | {favored_correct:>4} ({favored_correct/len(lost_genes)*100:>5.1f}%) | {favored_wrong:>4} ({favored_wrong/len(lost_genes)*100:>5.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Total weighted contribution\n",
    "    total_weighted = [sum([imm_weighted[i], codon_weighted[i], rbs_weighted[i], \n",
    "                           length_weighted[i], start_weighted[i]]) \n",
    "                      for i in range(len(lost_genes))]\n",
    "    print(f\"\\nTotal weighted advantage for wrong start: mean={np.mean(total_weighted):.3f}, median={np.median(total_weighted):.3f}\")\n",
    "    \n",
    "    # Point-based summary\n",
    "    correct_won_more = sum(1 for c in lost_genes if c['correct_points'] > c['selected_points'])\n",
    "    selected_won_more = sum(1 for c in lost_genes if c['selected_points'] > c['correct_points'])\n",
    "    tied = sum(1 for c in lost_genes if c['correct_points'] == c['selected_points'])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"POINT-BASED COMPARISON (1 point per component won)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Correct gene won MORE components:   {correct_won_more:>4} ({correct_won_more/len(lost_genes)*100:>5.1f}%)\")\n",
    "    print(f\"  Selected gene won MORE components:  {selected_won_more:>4} ({selected_won_more/len(lost_genes)*100:>5.1f}%)\")\n",
    "    print(f\"  Tied:                                {tied:>4} ({tied/len(lost_genes)*100:>5.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'fn_count': len(lost_genes),\n",
    "        'raw_diffs': {\n",
    "            'imm': np.mean(imm_raw),\n",
    "            'codon': np.mean(codon_raw),\n",
    "            'rbs': np.mean(rbs_raw),\n",
    "            'length': np.mean(length_raw),\n",
    "            'start': np.mean(start_raw)\n",
    "        },\n",
    "        'weighted_contributions': {\n",
    "            'imm': np.mean(imm_weighted),\n",
    "            'codon': np.mean(codon_weighted),\n",
    "            'rbs': np.mean(rbs_weighted),\n",
    "            'length': np.mean(length_weighted),\n",
    "            'start': np.mean(start_weighted)\n",
    "        },\n",
    "        'point_comparison': {\n",
    "            'correct_won_more': correct_won_more,\n",
    "            'selected_won_more': selected_won_more,\n",
    "            'tied': tied\n",
    "        }\n",
    "    }\n",
    "\n",
    "def analyze_negative_scores(all_orfs, reference_genes):\n",
    "    \"\"\"\n",
    "    Analyze how many TP and FP ORFs have negative scores,\n",
    "    both individually and in all possible combinations.\n",
    "    \n",
    "    Args:\n",
    "        all_orfs: All ORF candidates with scores\n",
    "        reference_genes: Set of (start, end) tuples for reference genes\n",
    "    \"\"\"\n",
    "    from itertools import combinations\n",
    "    \n",
    "    # Classify ORFs as TP or FP\n",
    "    print(\"=\"*80)\n",
    "    print(\"NEGATIVE SCORE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tp_orfs = []\n",
    "    fp_orfs = []\n",
    "    \n",
    "    for orf in all_orfs:\n",
    "        orf_start = orf.get('genome_start', orf['start'])\n",
    "        orf_end = orf.get('genome_end', orf['end'])\n",
    "        \n",
    "        if (orf_start, orf_end) in reference_genes:\n",
    "            tp_orfs.append(orf)\n",
    "        else:\n",
    "            fp_orfs.append(orf)\n",
    "    \n",
    "    print(f\"Total ORFs: {len(all_orfs):,}\")\n",
    "    print(f\"  True Positives: {len(tp_orfs):,}\")\n",
    "    print(f\"  False Positives: {len(fp_orfs):,}\")\n",
    "    \n",
    "    # Individual score analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INDIVIDUAL SCORES WITH NEGATIVE VALUES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    scores_to_check = ['codon_score', 'imm_score', 'length_score', 'rbs_score', 'start_score']\n",
    "    \n",
    "    print(f\"\\n{'Score':<20} {'TP Negative':>15} {'TP %':>10} {'FP Negative':>15} {'FP %':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for score_name in scores_to_check:\n",
    "        tp_negative = sum(1 for orf in tp_orfs if orf.get(score_name, 0) < 0)\n",
    "        fp_negative = sum(1 for orf in fp_orfs if orf.get(score_name, 0) < 0)\n",
    "        \n",
    "        tp_pct = tp_negative / len(tp_orfs) * 100 if tp_orfs else 0\n",
    "        fp_pct = fp_negative / len(fp_orfs) * 100 if fp_orfs else 0\n",
    "        \n",
    "        print(f\"{score_name:<20} {tp_negative:>15,} {tp_pct:>9.2f}% {fp_negative:>15,} {fp_pct:>9.2f}%\")\n",
    "    \n",
    "    # Two-score combinations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TWO-SCORE COMBINATIONS (BOTH NEGATIVE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    main_scores = ['codon_score', 'imm_score', 'length_score']\n",
    "    \n",
    "    print(f\"\\n{'Combination':<40} {'TP Count':>15} {'TP %':>10} {'FP Count':>15} {'FP %':>10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for score1, score2 in combinations(main_scores, 2):\n",
    "        tp_count = sum(1 for orf in tp_orfs \n",
    "                      if orf.get(score1, 0) < 0 and orf.get(score2, 0) < 0)\n",
    "        fp_count = sum(1 for orf in fp_orfs \n",
    "                      if orf.get(score1, 0) < 0 and orf.get(score2, 0) < 0)\n",
    "        \n",
    "        tp_pct = tp_count / len(tp_orfs) * 100 if tp_orfs else 0\n",
    "        fp_pct = fp_count / len(fp_orfs) * 100 if fp_orfs else 0\n",
    "        \n",
    "        combo_name = f\"{score1} & {score2}\"\n",
    "        print(f\"{combo_name:<40} {tp_count:>15,} {tp_pct:>9.2f}% {fp_count:>15,} {fp_pct:>9.2f}%\")\n",
    "    \n",
    "    # Three-score combination\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"THREE-SCORE COMBINATION (ALL VERY LOW)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tp_all_three = sum(1 for orf in tp_orfs \n",
    "                      if orf.get('codon_score', 0) < 0.2 \n",
    "                      and orf.get('imm_score', 0) < 0.2 \n",
    "                      and orf.get('length_score', 0) < 0.2)\n",
    "    fp_all_three = sum(1 for orf in fp_orfs \n",
    "                      if orf.get('codon_score', 0) < 0.2\n",
    "                      and orf.get('imm_score', 0) < 0.2 \n",
    "                      and orf.get('length_score', 0) < 0.2)\n",
    "    \n",
    "    tp_pct = tp_all_three / len(tp_orfs) * 100 if tp_orfs else 0\n",
    "    fp_pct = fp_all_three / len(fp_orfs) * 100 if fp_orfs else 0\n",
    "    \n",
    "    print(f\"\\n{'codon_score & imm_score & length_score':<40} {'TP Count':>15} {'TP %':>10} {'FP Count':>15} {'FP %':>10}\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'ALL THREE VERY LOW (<0.2)':<40} {tp_all_three:>15,} {tp_pct:>9.2f}% {fp_all_three:>15,} {fp_pct:>9.2f}%\")\n",
    "    \n",
    "    # Combined score analysis\n",
    "    if 'combined_score' in all_orfs[0]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMBINED SCORE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        tp_combined_neg = sum(1 for orf in tp_orfs if orf.get('combined_score', 0) < 0.2)\n",
    "        fp_combined_neg = sum(1 for orf in fp_orfs if orf.get('combined_score', 0) < 0.2)\n",
    "        \n",
    "        tp_pct = tp_combined_neg / len(tp_orfs) * 100 if tp_orfs else 0\n",
    "        fp_pct = fp_combined_neg / len(fp_orfs) * 100 if fp_orfs else 0\n",
    "        \n",
    "        print(f\"\\n{'Combined Score < 0.2':<40} {'TP Count':>15} {'TP %':>10} {'FP Count':>15} {'FP %':>10}\")\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"{'VERY LOW combined_score':<40} {tp_combined_neg:>15,} {tp_pct:>9.2f}% {fp_combined_neg:>15,} {fp_pct:>9.2f}%\")\n",
    "\n",
    "def diagnose_false_negatives(genome_id, cached_data, all_orfs, reference_genes, min_length=100):\n",
    "    \"\"\"\n",
    "    Analyze why reference genes are not being detected as ORFs.\n",
    "    \n",
    "    Args:\n",
    "        genome_id: Genome accession ID\n",
    "        cached_data: Cached genome data\n",
    "        all_orfs: All detected ORF candidates\n",
    "        reference_genes: Set of (start, end) tuples for reference genes\n",
    "        min_length: Minimum ORF length used in detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def reverse_complement(seq):\n",
    "        \"\"\"Simple reverse complement function.\"\"\"\n",
    "        complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G', 'N': 'N'}\n",
    "        return ''.join(complement.get(base, 'N') for base in reversed(seq))\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"FALSE NEGATIVE ANALYSIS - Why are genes missing?\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Get genome sequence\n",
    "    genome_sequence = cached_data[genome_id]['sequence']\n",
    "    \n",
    "    # Identify which genes are missing\n",
    "    detected_positions = set()\n",
    "    for orf in all_orfs:\n",
    "        orf_start = orf.get('genome_start', orf['start'])\n",
    "        orf_end = orf.get('genome_end', orf['end'])\n",
    "        detected_positions.add((orf_start, orf_end))\n",
    "    \n",
    "    missing_genes = reference_genes - detected_positions\n",
    "    found_genes = reference_genes & detected_positions\n",
    "    \n",
    "    print(f\"\\nTotal reference genes: {len(reference_genes):,}\")\n",
    "    print(f\"Detected as ORFs: {len(found_genes):,} ({len(found_genes)/len(reference_genes)*100:.1f}%)\")\n",
    "    print(f\"NOT detected (False Negatives): {len(missing_genes):,} ({len(missing_genes)/len(reference_genes)*100:.1f}%)\")\n",
    "    \n",
    "    if not missing_genes:\n",
    "        print(\"\\nAll reference genes were detected as ORFs\")\n",
    "        return\n",
    "    \n",
    "    # Load GFF for detailed analysis\n",
    "    gff_path = get_gff_path(genome_id)\n",
    "    print(f\"\\nLoading gene details from GFF...\")\n",
    "    \n",
    "    gff_df = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None,\n",
    "                         names=['seqid', 'source', 'type', 'start', 'end', 'score', 'strand', 'phase', 'attributes'])\n",
    "    \n",
    "    gff_df = gff_df[gff_df['type'] == 'CDS']\n",
    "    \n",
    "    # Create gene_details dictionary\n",
    "    gene_details = {}\n",
    "    for _, row in gff_df.iterrows():\n",
    "        start = int(row['start'])\n",
    "        end = int(row['end'])\n",
    "        \n",
    "        gene_details[(start, end)] = {\n",
    "            'length': end - start + 1,\n",
    "            'strand': row['strand'],\n",
    "            'start_pos': start,\n",
    "            'end_pos': end,\n",
    "            'attributes': row['attributes']\n",
    "        }\n",
    "    \n",
    "    # Categorize missing genes by reason\n",
    "    reasons = {\n",
    "        'too_short': [],\n",
    "        'no_start_codon': [],\n",
    "        'no_stop_codon': [],\n",
    "        'overlapping': [],\n",
    "        'frameshifted': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    start_codons = {'ATG', 'GTG', 'TTG'}\n",
    "    stop_codons = {'TAA', 'TAG', 'TGA'}\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(missing_genes):,} missing genes...\")\n",
    "    \n",
    "    for gene_pos in missing_genes:\n",
    "        if gene_pos not in gene_details:\n",
    "            reasons['other'].append({'position': gene_pos, 'reason': 'Not in GFF details'})\n",
    "            continue\n",
    "        \n",
    "        details = gene_details[gene_pos]\n",
    "        start = details['start_pos']\n",
    "        end = details['end_pos']\n",
    "        length = details['length']\n",
    "        strand = details['strand']\n",
    "        \n",
    "        # Check length\n",
    "        if length < min_length:\n",
    "            reasons['too_short'].append({\n",
    "                'position': gene_pos,\n",
    "                'length': length,\n",
    "                'strand': strand\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Extract sequence\n",
    "        if strand == '+':\n",
    "            gene_seq = genome_sequence[start-1:end]\n",
    "        else:\n",
    "            gene_seq = reverse_complement(genome_sequence[start-1:end])\n",
    "        \n",
    "        # Check for start codon\n",
    "        start_codon = gene_seq[:3]\n",
    "        has_start = start_codon in start_codons\n",
    "        \n",
    "        # Check for stop codon\n",
    "        stop_codon = gene_seq[-3:]\n",
    "        has_stop = stop_codon in stop_codons\n",
    "        \n",
    "        # Check if it's in frame\n",
    "        in_frame = (length % 3 == 0)\n",
    "        \n",
    "        if not has_start:\n",
    "            reasons['no_start_codon'].append({\n",
    "                'position': gene_pos,\n",
    "                'length': length,\n",
    "                'strand': strand,\n",
    "                'start_codon': start_codon\n",
    "            })\n",
    "        elif not has_stop:\n",
    "            reasons['no_stop_codon'].append({\n",
    "                'position': gene_pos,\n",
    "                'length': length,\n",
    "                'strand': strand,\n",
    "                'stop_codon': stop_codon\n",
    "            })\n",
    "        elif not in_frame:\n",
    "            reasons['frameshifted'].append({\n",
    "                'position': gene_pos,\n",
    "                'length': length,\n",
    "                'strand': strand\n",
    "            })\n",
    "        else:\n",
    "            # Check for overlaps\n",
    "            is_overlapping = False\n",
    "            for other_gene_pos in reference_genes:\n",
    "                if other_gene_pos == gene_pos:\n",
    "                    continue\n",
    "                other_start, other_end = other_gene_pos\n",
    "                if not (end < other_start or start > other_end):\n",
    "                    is_overlapping = True\n",
    "                    break\n",
    "            \n",
    "            if is_overlapping:\n",
    "                reasons['overlapping'].append({\n",
    "                    'position': gene_pos,\n",
    "                    'length': length,\n",
    "                    'strand': strand\n",
    "                })\n",
    "            else:\n",
    "                reasons['other'].append({\n",
    "                    'position': gene_pos,\n",
    "                    'length': length,\n",
    "                    'strand': strand,\n",
    "                    'start_codon': start_codon,\n",
    "                    'stop_codon': stop_codon\n",
    "                })\n",
    "    \n",
    "    # Report results\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"CATEGORIZATION OF FALSE NEGATIVES\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"\\n{'Category':<25} {'Count':>10} {'% of FN':>10} {'% of Total':>12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for category, genes in reasons.items():\n",
    "        count = len(genes)\n",
    "        pct_fn = count / len(missing_genes) * 100 if missing_genes else 0\n",
    "        pct_total = count / len(reference_genes) * 100\n",
    "        \n",
    "        category_name = category.replace('_', ' ').title()\n",
    "        print(f\"{category_name:<25} {count:>10,} {pct_fn:>9.1f}% {pct_total:>11.1f}%\")\n",
    "    \n",
    "    # Detailed breakdown\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED BREAKDOWN\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if reasons['too_short']:\n",
    "        print(f\"\\n1. TOO SHORT (< {min_length} bp): {len(reasons['too_short']):,} genes\")\n",
    "        lengths = [g['length'] for g in reasons['too_short']]\n",
    "        print(f\"   Length range: {min(lengths)}-{max(lengths)} bp\")\n",
    "        print(f\"   Mean length: {sum(lengths)/len(lengths):.1f} bp\")\n",
    "        print(f\"   [INFO] Solution: Consider lowering min_length threshold (currently {min_length})\")\n",
    "        \n",
    "        short_bins = [0, 50, 75, 100]\n",
    "        for i in range(len(short_bins)):\n",
    "            if i < len(short_bins) - 1:\n",
    "                count = sum(1 for l in lengths if short_bins[i] <= l < short_bins[i+1])\n",
    "                print(f\"      {short_bins[i]}-{short_bins[i+1]-1} bp: {count:,} genes\")\n",
    "            else:\n",
    "                count = sum(1 for l in lengths if l >= short_bins[i])\n",
    "                print(f\"      {short_bins[i]}+ bp: {count:,} genes\")\n",
    "    \n",
    "    if reasons['no_start_codon']:\n",
    "        print(f\"\\n2. NO STANDARD START CODON: {len(reasons['no_start_codon']):,} genes\")\n",
    "        start_codons_found = {}\n",
    "        for g in reasons['no_start_codon']:\n",
    "            codon = g['start_codon']\n",
    "            start_codons_found[codon] = start_codons_found.get(codon, 0) + 1\n",
    "        print(\"   Start codons found:\")\n",
    "        for codon, count in sorted(start_codons_found.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"      {codon}: {count:,}\")\n",
    "        print(f\"   [INFO] Solution: These genes use non-standard start codons\")\n",
    "        print(f\"      Consider adding alternative start codons to detection\")\n",
    "    \n",
    "    if reasons['no_stop_codon']:\n",
    "        print(f\"\\n3. NO STANDARD STOP CODON: {len(reasons['no_stop_codon']):,} genes\")\n",
    "        print(f\"   [INFO] Solution: These genes may extend beyond expected boundaries\")\n",
    "        print(f\"      or have annotation errors\")\n",
    "    \n",
    "    if reasons['frameshifted']:\n",
    "        print(f\"\\n4. NOT IN READING FRAME: {len(reasons['frameshifted']):,} genes\")\n",
    "        print(f\"   [INFO] Solution: These genes may have programmed frameshifts\")\n",
    "        print(f\"      or annotation errors\")\n",
    "    \n",
    "    if reasons['overlapping']:\n",
    "        print(f\"\\n5. OVERLAPPING WITH OTHER GENES: {len(reasons['overlapping']):,} genes\")\n",
    "        print(f\"   [INFO] Solution: These genes overlap with others on the same or opposite strand\")\n",
    "        print(f\"      Your ORF detection may be choosing one over the other\")\n",
    "    \n",
    "    if reasons['other']:\n",
    "        print(f\"\\n6. OTHER REASONS: {len(reasons['other']):,} genes\")\n",
    "        print(f\"   These genes have standard start/stop codons and correct length\")\n",
    "        print(f\"   but are still not detected. Requires manual investigation.\")\n",
    "        \n",
    "        print(f\"\\n   Sample cases:\")\n",
    "        for i, g in enumerate(reasons['other'][:5], 1):\n",
    "            pos = g['position']\n",
    "            print(f\"      {i}. Position {pos[0]:,}-{pos[1]:,}, {g['length']} bp, strand {g['strand']}\")\n",
    "            print(f\"         Start: {g.get('start_codon', 'N/A')}, Stop: {g.get('stop_codon', 'N/A')}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "861a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_genome(genome_id, cached_data):\n",
    "    \"\"\"Process a single genome through the ORF prediction pipeline.\"\"\"\n",
    "    genome_data = cached_data[genome_id]\n",
    "    all_orfs = genome_data['orfs']\n",
    "    \n",
    "    training_set = create_training_set(genome_id, cached_data)\n",
    "    intergenic_set = create_intergenic_set(genome_id, cached_data)\n",
    "    \n",
    "    models = build_all_scoring_models(training_set, intergenic_set)\n",
    "    \n",
    "    scored_orfs = score_all_orfs(all_orfs, models)\n",
    "    scored_orfs = normalize_all_orf_scores(scored_orfs)\n",
    "    scored_orfs = add_combined_scores(scored_orfs)\n",
    "    \n",
    "    candidates = filter_candidates(scored_orfs)\n",
    "    grouped_orfs = organize_nested_orfs(candidates)\n",
    "    top_candidates = select_best_starts(grouped_orfs)\n",
    "    top_candidates = filter_candidates(top_candidates, 0.2, 0.2, 0.2, 0)\n",
    "    \n",
    "    compare_orfs_to_reference(top_candidates, genome_id)\n",
    "    \n",
    "    return top_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aebedb",
   "metadata": {},
   "source": [
    "#TODO attempt to improve scoring efficieny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8dfa5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "# Global cache variables - MUST match traditional_methods.py\n",
    "_GLOBAL_CODING_IMM = None\n",
    "_GLOBAL_NONCODING_IMM = None\n",
    "def clear_imm_cache():\n",
    "    \"\"\"Clear the LRU cache for IMM scoring.\"\"\"\n",
    "    get_interpolated_probability_cached.cache_clear()\n",
    "\n",
    "def build_all_scoring_models_cached(\n",
    "    training_set: List[Dict], \n",
    "    intergenic_set: List[Dict], \n",
    "    min_observations: int = 10\n",
    ") -> Dict:\n",
    "    \"\"\"Build all traditional scoring models from training data.\"\"\"\n",
    "    print(\"Building traditional scoring models...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    clear_imm_cache()\n",
    "    \n",
    "    print(\"  Building codon usage models...\")\n",
    "    codon_model = build_codon_model(training_set)\n",
    "    background_codon_model = build_codon_model(intergenic_set)\n",
    "\n",
    "    print(\"  Building IMM models...\")\n",
    "    training_seqs = [orf['sequence'] for orf in training_set]\n",
    "    intergenic_seqs = [orf['sequence'] for orf in intergenic_set]\n",
    "    \n",
    "    n_training = sum(len(seq) for seq in training_seqs)\n",
    "    n_intergenic = sum(len(seq) for seq in intergenic_seqs)\n",
    "    effective_n = min(n_training, n_intergenic)\n",
    "    \n",
    "    if effective_n < min_observations:\n",
    "        estimated_order = 0\n",
    "    else:\n",
    "        estimated_order = math.floor(math.log2(effective_n / min_observations) / 2)\n",
    "    \n",
    "    estimated_order = min(estimated_order, 8)\n",
    "    estimated_order = max(estimated_order, 3)\n",
    "    \n",
    "    coding_imm = build_interpolated_markov_model(training_seqs, estimated_order, min_observations)\n",
    "    noncoding_imm = build_interpolated_markov_model(intergenic_seqs, estimated_order, min_observations)\n",
    "    \n",
    "    print(f\"✓ All models built in {time.time() - start_time:.1f}s\")\n",
    "    print(f\"  IMM order: {estimated_order}\")\n",
    "    print(f\"  Training sequences: {len(training_seqs)} ({n_training:,} bp)\")\n",
    "    print(f\"  Intergenic sequences: {len(intergenic_seqs)} ({n_intergenic:,} bp)\")\n",
    "    \n",
    "    return {\n",
    "        'codon_model': codon_model,\n",
    "        'background_codon_model': background_codon_model,\n",
    "        'coding_imm': coding_imm,\n",
    "        'noncoding_imm': noncoding_imm,\n",
    "        'max_order': estimated_order\n",
    "    }\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=200000)  \n",
    "def get_interpolated_probability_cached(\n",
    "    nucleotide: str,\n",
    "    context: str,\n",
    "    codon_pos: int,  \n",
    "    imm_type: str,   \n",
    "    fallback_prob: float = 0.25\n",
    ") -> float:\n",
    "\n",
    "    global _GLOBAL_CODING_IMM, _GLOBAL_NONCODING_IMM  \n",
    "    \n",
    "    probabilities = _GLOBAL_CODING_IMM if imm_type == 'coding' else _GLOBAL_NONCODING_IMM\n",
    "    \n",
    "    \n",
    "    for order in range(len(context), -1, -1):\n",
    "        current_context = context[-order:] if order > 0 else \"\"\n",
    "        \n",
    "        \n",
    "        if current_context in probabilities[codon_pos]:\n",
    "            if nucleotide in probabilities[codon_pos][current_context]:\n",
    "                return probabilities[codon_pos][current_context][nucleotide]\n",
    "\n",
    "    return fallback_prob\n",
    "\n",
    "def score_imm_ratio_cached(\n",
    "    sequence: str, \n",
    "    coding_imm: List[Dict], \n",
    "    noncoding_imm: List[Dict], \n",
    "    max_order: int\n",
    ") -> float:\n",
    "    \"\"\"Score sequence using frame-aware IMM log-likelihood ratio.\"\"\"\n",
    "    global _GLOBAL_CODING_IMM, _GLOBAL_NONCODING_IMM \n",
    "    \n",
    "    _GLOBAL_CODING_IMM = coding_imm\n",
    "    _GLOBAL_NONCODING_IMM = noncoding_imm\n",
    "    \n",
    "    if len(sequence) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    is_frame_aware = isinstance(coding_imm, list) and len(coding_imm) == 3\n",
    "    \n",
    "    coding_log_prob = 0.0\n",
    "    noncoding_log_prob = 0.0\n",
    "    EPSILON = 1e-10\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        nucleotide = sequence[i]\n",
    "        \n",
    "        context_start = max(0, i - max_order)\n",
    "        context = sequence[context_start:i]\n",
    "        \n",
    "        if is_frame_aware:\n",
    "            codon_position = i % 3\n",
    "            coding_prob = get_interpolated_probability_cached(\n",
    "                nucleotide, context, codon_position, 'coding'\n",
    "            )\n",
    "            noncoding_prob = get_interpolated_probability_cached(\n",
    "                nucleotide, context, codon_position, 'noncoding'\n",
    "            )\n",
    "        else:\n",
    "            coding_prob = get_interpolated_probability_cached(\n",
    "                nucleotide, context, 0, 'coding'\n",
    "            )\n",
    "            noncoding_prob = get_interpolated_probability_cached(\n",
    "                nucleotide, context, 0, 'noncoding'\n",
    "            )\n",
    "        \n",
    "        coding_prob = max(coding_prob, EPSILON)\n",
    "        noncoding_prob = max(noncoding_prob, EPSILON)\n",
    "        \n",
    "        coding_log_prob += math.log(coding_prob)\n",
    "        noncoding_log_prob += math.log(noncoding_prob)\n",
    "    \n",
    "    return (coding_log_prob - noncoding_log_prob) / len(sequence)\n",
    "\n",
    "def score_all_orfs_cached(all_orfs, models):\n",
    "    \"\"\"\n",
    "    Score all ORFs with all methods using pre-built models.\n",
    "    Adds all scores and RBS details to each ORF dict.\n",
    "    \n",
    "    Args:\n",
    "        all_orfs (list): List of ORF dictionaries\n",
    "        models (dict): Pre-built models from build_all_scoring_models()\n",
    "        \n",
    "    Returns:\n",
    "        list: ORFs with scores added\n",
    "    \"\"\"\n",
    "    print(f\"Scoring {len(all_orfs):,} ORFs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    codon_model = models['codon_model']\n",
    "    background_codon_model = models['background_codon_model']\n",
    "    coding_imm = models['coding_imm']\n",
    "    noncoding_imm = models['noncoding_imm']\n",
    "    max_order = models['max_order']\n",
    "    \n",
    "    for i, orf in enumerate(all_orfs):\n",
    "        if i % 25000 == 0 and i > 0:\n",
    "            print(f\"  {i:,}...\")\n",
    "        \n",
    "        # Score each component\n",
    "        orf['codon_score'] = score_codon_bias_ratio(\n",
    "            orf['sequence'], \n",
    "            codon_model, \n",
    "            background_codon_model\n",
    "        )\n",
    "        \n",
    "        orf['imm_score'] = score_imm_ratio_cached(\n",
    "            orf['sequence'], \n",
    "            coding_imm, \n",
    "            noncoding_imm,\n",
    "            max_order\n",
    "        )\n",
    "        # Length score\n",
    "        orf['length_score'] = score_orf_length(orf['length'])\n",
    "        \n",
    "        # Start codon score\n",
    "        orf['start_score'] = score_start_codon(orf.get('start_codon', 'ATG'))\n",
    "    \n",
    "    print(f\"Done in {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    return all_orfs\n",
    "\n",
    "def process_genome_cached(genome_id, cached_data):\n",
    "    \"\"\"\n",
    "    Process genome with LRU cached IMM scoring.\n",
    "    Drop-in replacement that works with your loop.\n",
    "    \"\"\"\n",
    "    genome_data = cached_data[genome_id]\n",
    "    all_orfs = genome_data['orfs']\n",
    "    \n",
    "    # Create training sets\n",
    "    training_set = create_training_set(genome_id, cached_data)\n",
    "    intergenic_set = create_intergenic_set(genome_id, cached_data)\n",
    "    \n",
    "    # Build models\n",
    "    models = build_all_scoring_models_cached(training_set, intergenic_set)\n",
    "    \n",
    "    scored_orfs=score_all_orfs_cached(all_orfs, models)\n",
    "    # Normalize and combine - use all_orfs since we scored in-place\n",
    "    scored_orfs = normalize_all_orf_scores(scored_orfs)\n",
    "    scored_orfs = add_combined_scores(scored_orfs)\n",
    "    \n",
    "    # Filter and select\n",
    "    candidates = filter_candidates(scored_orfs)\n",
    "    grouped_orfs = organize_nested_orfs(candidates)\n",
    "    top_candidates = select_best_starts(grouped_orfs)\n",
    "    final_predictions = filter_candidates(top_candidates, 0.2, 0.2, 0.2, 0)\n",
    "    \n",
    "    # Get metrics\n",
    "    metrics = compare_orfs_to_reference(final_predictions, genome_id)\n",
    "    \n",
    "    return {\n",
    "        'predictions': final_predictions,\n",
    "        'metrics': metrics,\n",
    "        'count': len(final_predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42347a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 2.6s\n",
      "  IMM order: 6\n",
      "  Training sequences: 1220 (1,809,003 bp)\n",
      "  Intergenic sequences: 490 (148,082 bp)\n",
      "Scoring 176,315 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "  175,000...\n",
      "Done in 1.1 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "\n",
      "Selecting best start for 9,972\n",
      "================================================================================\n",
      "VALIDATION SUMMARY: NC_000913.3\n",
      "================================================================================\n",
      "Predicted ORFs:              5,687\n",
      "Reference CDS (proteins):    4,340\n",
      "True positives (exact):      3,427\n",
      "False negatives (missed):    913\n",
      "False positives (spurious):  2,260\n",
      "\n",
      "Sensitivity (Recall):        78.96%\n",
      "Precision:                   60.26%\n",
      "F1 Score:                    68.36\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "predictions = process_genome_cached(genome_id, cached_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873f78e",
   "metadata": {},
   "source": [
    "WEIGHTS OPTIMIZATION- 2 hours run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0931aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "def objective_function(weight_array, grouped_orfs, ref_set):\n",
    "    \"\"\"\n",
    "    Objective function: Returns NEGATIVE accuracy (to minimize).\n",
    "    \n",
    "    Args:\n",
    "        weight_array: [codon, imm, rbs, length, start]\n",
    "        grouped_orfs: The grouped ORFs\n",
    "        ref_set: Set of reference gene coordinates\n",
    "    \n",
    "    Returns:\n",
    "        -accuracy (negative so we can minimize)\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        'codon': weight_array[0],\n",
    "        'imm': weight_array[1],\n",
    "        'rbs': weight_array[2],\n",
    "        'length': weight_array[3],\n",
    "        'start': weight_array[4]\n",
    "    }\n",
    "    \n",
    "    def weighted_score(orf):\n",
    "        return (\n",
    "            orf['codon_score_norm'] * weights['codon'] +\n",
    "            orf['imm_score_norm'] * weights['imm'] +\n",
    "            orf['rbs_score_norm'] * weights['rbs'] +\n",
    "            orf['length_score_norm'] * weights['length'] +\n",
    "            orf['start_score_norm'] * weights['start']\n",
    "        )\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for stop_pos, orf_group in grouped_orfs.items():\n",
    "        if len(orf_group) < 2:\n",
    "            continue\n",
    "        \n",
    "        correct_orf = None\n",
    "        for orf in orf_group:\n",
    "            coord = (orf.get('genome_start', orf['start']), \n",
    "                    orf.get('genome_end', orf['end']))\n",
    "            if coord in ref_set:\n",
    "                correct_orf = orf\n",
    "                break\n",
    "        \n",
    "        if correct_orf is None:\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "        selected = max(orf_group, key=weighted_score)\n",
    "        selected_coord = (selected.get('genome_start', selected['start']),\n",
    "                         selected.get('genome_end', selected['end']))\n",
    "        correct_coord = (correct_orf.get('genome_start', correct_orf['start']),\n",
    "                        correct_orf.get('genome_end', correct_orf['end']))\n",
    "        \n",
    "        if selected_coord == correct_coord:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return -accuracy  # Negative because we minimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226975d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                              BEST PRACTICE WEIGHT OPTIMIZATION\n",
      "====================================================================================================\n",
      "\n",
      "STEP 1: PREPARING DATA\n",
      "================================================================================\n",
      "PREPARING ALL GENOME DATA\n",
      "================================================================================\n",
      "\n",
      "Processing NC_000913.3...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 2.5s\n",
      "  IMM order: 6\n",
      "  Training sequences: 1220 (1,809,003 bp)\n",
      "  Intergenic sequences: 490 (148,082 bp)\n",
      "Scoring 176,315 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "  175,000...\n",
      "Done in 1.0 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 9972 ORF groups prepared\n",
      "\n",
      "Processing NC_000964.3...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 4.0s\n",
      "  IMM order: 6\n",
      "  Training sequences: 1187 (1,605,582 bp)\n",
      "  Intergenic sequences: 437 (133,496 bp)\n",
      "Scoring 139,046 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "Done in 1.7 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 8545 ORF groups prepared\n",
      "\n",
      "Processing NC_003197.2...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 5.9s\n",
      "  IMM order: 7\n",
      "  Training sequences: 1252 (1,882,869 bp)\n",
      "  Intergenic sequences: 531 (167,961 bp)\n",
      "Scoring 173,564 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "Done in 1.8 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 10294 ORF groups prepared\n",
      "\n",
      "Processing NC_002505.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 2.3s\n",
      "  IMM order: 6\n",
      "  Training sequences: 1155 (1,410,936 bp)\n",
      "  Intergenic sequences: 339 (111,780 bp)\n",
      "Scoring 119,640 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "Done in 0.8 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 5185 ORF groups prepared\n",
      "\n",
      "Processing NC_000962.3...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 1.5s\n",
      "  IMM order: 4\n",
      "  Training sequences: 624 (1,020,396 bp)\n",
      "  Intergenic sequences: 26 (6,300 bp)\n",
      "Scoring 238,665 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "  175,000...\n",
      "  200,000...\n",
      "  225,000...\n",
      "Done in 1.9 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 14967 ORF groups prepared\n",
      "\n",
      "Processing NC_002695.2...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 6.1s\n",
      "  IMM order: 7\n",
      "  Training sequences: 1247 (1,990,818 bp)\n",
      "  Intergenic sequences: 580 (183,861 bp)\n",
      "Scoring 206,168 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "  175,000...\n",
      "  200,000...\n",
      "Done in 1.9 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 12105 ORF groups prepared\n",
      "\n",
      "Processing NC_008253.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 3.1s\n",
      "  IMM order: 7\n",
      "  Training sequences: 1228 (1,872,852 bp)\n",
      "  Intergenic sequences: 520 (165,491 bp)\n",
      "Scoring 186,313 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "  175,000...\n",
      "Done in 1.9 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 10388 ORF groups prepared\n",
      "\n",
      "Processing NC_000915.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 1.3s\n",
      "  IMM order: 6\n",
      "  Training sequences: 834 (916,362 bp)\n",
      "  Intergenic sequences: 209 (69,732 bp)\n",
      "Scoring 55,196 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "Done in 0.6 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 1960 ORF groups prepared\n",
      "\n",
      "Processing NC_003210.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 3.2s\n",
      "  IMM order: 6\n",
      "  Training sequences: 1181 (1,390,221 bp)\n",
      "  Intergenic sequences: 340 (96,321 bp)\n",
      "Scoring 76,931 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "Done in 0.7 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 3877 ORF groups prepared\n",
      "\n",
      "Processing NC_002516.2...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 2.4s\n",
      "  IMM order: 5\n",
      "  Training sequences: 601 (1,336,896 bp)\n",
      "  Intergenic sequences: 55 (16,787 bp)\n",
      "Scoring 284,420 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "  150,000...\n",
      "  175,000...\n",
      "  200,000...\n",
      "  225,000...\n",
      "  250,000...\n",
      "  275,000...\n",
      "Done in 2.0 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 16377 ORF groups prepared\n",
      "\n",
      "Processing NC_000854.2...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 0.5s\n",
      "  IMM order: 6\n",
      "  Training sequences: 425 (404,340 bp)\n",
      "  Intergenic sequences: 137 (42,892 bp)\n",
      "Scoring 49,849 ORFs...\n",
      "  25,000...\n",
      "Done in 0.3 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 3133 ORF groups prepared\n",
      "\n",
      "Processing NC_000868.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 0.8s\n",
      "  IMM order: 5\n",
      "  Training sequences: 728 (732,666 bp)\n",
      "  Intergenic sequences: 101 (33,406 bp)\n",
      "Scoring 44,536 ORFs...\n",
      "  25,000...\n",
      "Done in 0.2 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 2606 ORF groups prepared\n",
      "\n",
      "Processing NC_002607.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 0.4s\n",
      "  IMM order: 4\n",
      "  Training sequences: 353 (478,500 bp)\n",
      "  Intergenic sequences: 26 (6,495 bp)\n",
      "Scoring 96,911 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "Done in 0.7 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 6307 ORF groups prepared\n",
      "\n",
      "Processing NC_003552.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 9.0s\n",
      "  IMM order: 8\n",
      "  Training sequences: 1237 (1,904,319 bp)\n",
      "  Intergenic sequences: 1758 (711,381 bp)\n",
      "Scoring 147,895 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "  75,000...\n",
      "  100,000...\n",
      "  125,000...\n",
      "Done in 1.9 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 13186 ORF groups prepared\n",
      "\n",
      "Processing NC_000917.1...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 1.0s\n",
      "  IMM order: 5\n",
      "  Training sequences: 861 (959,463 bp)\n",
      "  Intergenic sequences: 116 (34,444 bp)\n",
      "Scoring 70,429 ORFs...\n",
      "  25,000...\n",
      "  50,000...\n",
      "Done in 0.3 minutes\n",
      "\n",
      "Normalizing ORFs...\n",
      "\n",
      "Calculating weighted scores...\n",
      "  ✓ 4804 ORF groups prepared\n",
      "\n",
      "================================================================================\n",
      "Total genomes prepared: 15\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "STEP 2: TRAIN/VALIDATION SPLIT\n",
      "================================================================================\n",
      "OPTION C: TRAIN/VALIDATION SPLIT OPTIMIZATION\n",
      "================================================================================\n",
      "Total genomes: 15\n",
      "Training genomes (10): ['NC_003210.1', 'NC_003552.1', 'NC_000915.1', 'NC_008253.1', 'NC_000917.1', 'NC_002607.1', 'NC_002695.2', 'NC_003197.2', 'NC_002516.2', 'NC_002505.1']\n",
      "Validation genomes (5): ['NC_000962.3', 'NC_000868.1', 'NC_000913.3', 'NC_000964.3', 'NC_000854.2']\n",
      "================================================================================\n",
      "\n",
      "PHASE 1: OPTIMIZING ON TRAINING SET\n",
      "--------------------------------------------------------------------------------\n",
      "Iter 1: Train accuracy = 85.05% [codon=2.84, imm=5.10, rbs=0.77, length=9.81, start=0.22]\n",
      "Iter 3: Train accuracy = 85.34% [codon=3.10, imm=3.70, rbs=0.60, length=7.17, start=0.32]\n",
      "Iter 4: Train accuracy = 85.51% [codon=2.33, imm=2.58, rbs=0.99, length=7.44, start=0.35]\n",
      "Iter 7: Train accuracy = 85.68% [codon=3.09, imm=4.18, rbs=0.99, length=9.81, start=0.34]\n",
      "Iter 8: Train accuracy = 85.98% [codon=3.44, imm=2.34, rbs=0.58, length=7.33, start=0.30]\n",
      "Iter 9: Train accuracy = 86.16% [codon=3.66, imm=1.26, rbs=0.59, length=6.57, start=0.28]\n",
      "Iter 13: Train accuracy = 86.23% [codon=3.85, imm=1.78, rbs=0.72, length=7.09, start=0.32]\n",
      "Iter 15: Train accuracy = 86.26% [codon=3.66, imm=1.08, rbs=0.58, length=5.62, start=0.18]\n",
      "Iter 19: Train accuracy = 86.32% [codon=3.65, imm=1.03, rbs=0.58, length=5.69, start=0.26]\n",
      "Iter 23: Train accuracy = 86.32% [codon=3.66, imm=0.92, rbs=0.56, length=5.43, start=0.19]\n",
      "Iter 41: Train accuracy = 86.34% [codon=3.31, imm=0.48, rbs=0.41, length=4.65, start=0.17]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 394\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m weights_trainval, val_acc\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# RUN THE COMPLETE BEST PRACTICE OPTIMIZATION\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# 4. Train on all data if generalization is good\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# 5. Return best weights\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m final_weights, final_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_best_practice_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_GENOMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 342\u001b[0m, in \u001b[0;36mcombined_best_practice_optimization\u001b[1;34m(TEST_GENOMES, cached_data)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSTEP 2: TRAIN/VALIDATION SPLIT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m weights_trainval, train_acc, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_with_train_val_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_genome_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Step 3: Decision point\u001b[39;00m\n\u001b[0;32m    345\u001b[0m generalization_gap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(train_acc \u001b[38;5;241m-\u001b[39m val_acc)\n",
      "Cell \u001b[1;32mIn[92], line 178\u001b[0m, in \u001b[0;36moptimize_with_train_val_split\u001b[1;34m(all_genome_data, train_ratio)\u001b[0m\n\u001b[0;32m    174\u001b[0m         train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcurrent_score \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[codon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxk[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, imm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxk[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, rbs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxk[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, length=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxk[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, start=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxk[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m result_train \u001b[38;5;241m=\u001b[39m \u001b[43mdifferential_evolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function_multi_genome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_genome_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest1bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpopsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmutation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecombination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolish\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeferred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m optimal_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodon\u001b[39m\u001b[38;5;124m'\u001b[39m: result_train\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimm\u001b[39m\u001b[38;5;124m'\u001b[39m: result_train\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m: result_train\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m    200\u001b[0m }\n\u001b[0;32m    202\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mresult_train\u001b[38;5;241m.\u001b[39mfun \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gene_prediction\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py:502\u001b[0m, in \u001b[0;36mdifferential_evolution\u001b[1;34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, seed, callback, disp, polish, init, atol, updating, workers, constraints, x0, integrality, vectorized)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# using a context manager means that any created Pool objects are\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# cleared up.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DifferentialEvolutionSolver(func, bounds, args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    488\u001b[0m                                  strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    489\u001b[0m                                  maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    500\u001b[0m                                  integrality\u001b[38;5;241m=\u001b[39mintegrality,\n\u001b[0;32m    501\u001b[0m                                  vectorized\u001b[38;5;241m=\u001b[39mvectorized) \u001b[38;5;28;01mas\u001b[39;00m solver:\n\u001b[1;32m--> 502\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gene_prediction\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py:1164\u001b[0m, in \u001b[0;36mDifferentialEvolutionSolver.solve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxiter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m# evolve the population by a generation\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1164\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m         warning_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gene_prediction\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py:1618\u001b[0m, in \u001b[0;36mDifferentialEvolutionSolver.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1615\u001b[0m trial_energies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_population_members, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;66;03m# only calculate for feasible entries\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m trial_energies[feasible] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_population_energies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_pop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeasible\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;66;03m# which solutions are 'improved'?\u001b[39;00m\n\u001b[0;32m   1622\u001b[0m loc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accept_trial(\u001b[38;5;241m*\u001b[39mval) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m        \u001b[38;5;28mzip\u001b[39m(trial_energies, feasible, cv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_energies,\n\u001b[0;32m   1624\u001b[0m            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeasible, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraint_violation)]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gene_prediction\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py:1315\u001b[0m, in \u001b[0;36mDifferentialEvolutionSolver._calculate_population_energies\u001b[1;34m(self, population)\u001b[0m\n\u001b[0;32m   1313\u001b[0m parameters_pop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_parameters(population)\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1315\u001b[0m     calc_energies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters_pop\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     calc_energies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(calc_energies)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;66;03m# wrong number of arguments for _mapwrapper\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;66;03m# or wrong length returned from the mapper\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gene_prediction\\lib\\site-packages\\scipy\\_lib\\_util.py:441\u001b[0m, in \u001b[0;36m_FunctionWrapper.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 110\u001b[0m, in \u001b[0;36mobjective_function_multi_genome\u001b[1;34m(weight_array, genome_data_dict, genome_ids)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 110\u001b[0m selected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43morf_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m selected_coord \u001b[38;5;241m=\u001b[39m (selected\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenome_start\u001b[39m\u001b[38;5;124m'\u001b[39m, selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    112\u001b[0m                  selected\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenome_end\u001b[39m\u001b[38;5;124m'\u001b[39m, selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    113\u001b[0m correct_coord \u001b[38;5;241m=\u001b[39m (correct_orf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenome_start\u001b[39m\u001b[38;5;124m'\u001b[39m, correct_orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    114\u001b[0m                 correct_orf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenome_end\u001b[39m\u001b[38;5;124m'\u001b[39m, correct_orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[92], line 76\u001b[0m, in \u001b[0;36mobjective_function_multi_genome.<locals>.weighted_score\u001b[1;34m(orf)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mweighted_score\u001b[39m(orf):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 76\u001b[0m         \u001b[43morf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcodon_score_norm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     77\u001b[0m         orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimm_score_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     78\u001b[0m         orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbs_score_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     79\u001b[0m         orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_score_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     80\u001b[0m         orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_score_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     81\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_all_genome_data(TEST_GENOMES, cached_data):\n",
    "    \"\"\"\n",
    "    Prepare grouped ORFs and reference sets for all genomes.\n",
    "    Returns dict with genome_id -> (grouped_orfs, ref_set)\n",
    "    \"\"\"\n",
    "    from src.comparative_analysis import get_gff_path\n",
    "    \n",
    "    all_genome_data = {}\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"PREPARING ALL GENOME DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for gid in TEST_GENOMES:\n",
    "        print(f\"\\nProcessing {gid}...\")\n",
    "        \n",
    "        # Run pipeline to get grouped ORFs\n",
    "        genome_data = cached_data[gid]\n",
    "        all_orfs = genome_data['orfs']\n",
    "        \n",
    "        training_set = create_training_set(gid, cached_data)\n",
    "        intergenic_set = create_intergenic_set(gid, cached_data)\n",
    "        models = build_all_scoring_models_cached(training_set, intergenic_set)\n",
    "        \n",
    "        scored_orfs = score_all_orfs_cached(all_orfs, models)\n",
    "        scored_orfs = normalize_all_orf_scores(scored_orfs)\n",
    "        scored_orfs = add_combined_scores(scored_orfs)\n",
    "        \n",
    "        candidates = filter_candidates(scored_orfs)\n",
    "        grouped_orfs = organize_nested_orfs(candidates)\n",
    "        \n",
    "        # Load reference\n",
    "        gff_path = get_gff_path(gid)\n",
    "        ref = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None)\n",
    "        ref_genes = ref[ref[2] == \"CDS\"][[3, 4]]\n",
    "        ref_genes.columns = [\"start\", \"end\"]\n",
    "        ref_set = set(zip(ref_genes['start'], ref_genes['end']))\n",
    "        \n",
    "        all_genome_data[gid] = {\n",
    "            'grouped_orfs': grouped_orfs,\n",
    "            'ref_set': ref_set\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ {len(grouped_orfs)} ORF groups prepared\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Total genomes prepared: {len(all_genome_data)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return all_genome_data\n",
    "\n",
    "\n",
    "def objective_function_multi_genome(weight_array, genome_data_dict, genome_ids):\n",
    "    \"\"\"\n",
    "    Objective function for multiple genomes.\n",
    "    Returns average negative accuracy across all genomes.\n",
    "    \n",
    "    Args:\n",
    "        weight_array: [codon, imm, rbs, length, start]\n",
    "        genome_data_dict: Dict of genome_id -> {grouped_orfs, ref_set}\n",
    "        genome_ids: List of genome IDs to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        -average_accuracy (negative for minimization)\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        'codon': weight_array[0],\n",
    "        'imm': weight_array[1],\n",
    "        'rbs': weight_array[2],\n",
    "        'length': weight_array[3],\n",
    "        'start': weight_array[4]\n",
    "    }\n",
    "    \n",
    "    def weighted_score(orf):\n",
    "        return (\n",
    "            orf['codon_score_norm'] * weights['codon'] +\n",
    "            orf['imm_score_norm'] * weights['imm'] +\n",
    "            orf['rbs_score_norm'] * weights['rbs'] +\n",
    "            orf['length_score_norm'] * weights['length'] +\n",
    "            orf['start_score_norm'] * weights['start']\n",
    "        )\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for gid in genome_ids:\n",
    "        grouped_orfs = genome_data_dict[gid]['grouped_orfs']\n",
    "        ref_set = genome_data_dict[gid]['ref_set']\n",
    "        \n",
    "        correct = 0\n",
    "        count = 0\n",
    "        \n",
    "        for stop_pos, orf_group in grouped_orfs.items():\n",
    "            if len(orf_group) < 2:\n",
    "                continue\n",
    "            \n",
    "            correct_orf = None\n",
    "            for orf in orf_group:\n",
    "                coord = (orf.get('genome_start', orf['start']), \n",
    "                        orf.get('genome_end', orf['end']))\n",
    "                if coord in ref_set:\n",
    "                    correct_orf = orf\n",
    "                    break\n",
    "            \n",
    "            if correct_orf is None:\n",
    "                continue\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            selected = max(orf_group, key=weighted_score)\n",
    "            selected_coord = (selected.get('genome_start', selected['start']),\n",
    "                             selected.get('genome_end', selected['end']))\n",
    "            correct_coord = (correct_orf.get('genome_start', correct_orf['start']),\n",
    "                            correct_orf.get('genome_end', correct_orf['end']))\n",
    "            \n",
    "            if selected_coord == correct_coord:\n",
    "                correct += 1\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_count += count\n",
    "    \n",
    "    average_accuracy = total_correct / total_count if total_count > 0 else 0\n",
    "    return -average_accuracy  # Negative for minimization\n",
    "\n",
    "\n",
    "def optimize_with_train_val_split(all_genome_data, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    OPTION C: Train/Validation Split with proper ML methodology.\n",
    "    \n",
    "    Splits genomes into train/validation sets, optimizes on train,\n",
    "    evaluates on validation to check generalization.\n",
    "    \"\"\"\n",
    "    genome_ids = list(all_genome_data.keys())\n",
    "    n_train = int(len(genome_ids) * train_ratio)\n",
    "    \n",
    "    # Shuffle and split\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    shuffled_ids = genome_ids.copy()\n",
    "    random.shuffle(shuffled_ids)\n",
    "    \n",
    "    train_ids = shuffled_ids[:n_train]\n",
    "    val_ids = shuffled_ids[n_train:]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTION C: TRAIN/VALIDATION SPLIT OPTIMIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total genomes: {len(genome_ids)}\")\n",
    "    print(f\"Training genomes ({len(train_ids)}): {train_ids}\")\n",
    "    print(f\"Validation genomes ({len(val_ids)}): {val_ids}\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Optimize on training set\n",
    "    print(\"PHASE 1: OPTIMIZING ON TRAINING SET\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    bounds = [\n",
    "        (0.1, 5.0),  # codon\n",
    "        (0.1, 10.0), # imm\n",
    "        (0.1, 5.0),  # rbs\n",
    "        (0.1, 10.0), # length\n",
    "        (0.1, 3.0),  # start\n",
    "    ]\n",
    "    \n",
    "    iteration = [0]\n",
    "    best_train = [float('inf')]\n",
    "    \n",
    "    def callback_train(xk, convergence):\n",
    "        iteration[0] += 1\n",
    "        current_score = objective_function_multi_genome(xk, all_genome_data, train_ids)\n",
    "        if current_score < best_train[0]:\n",
    "            best_train[0] = current_score\n",
    "            train_acc = -current_score * 100\n",
    "            print(f\"Iter {iteration[0]}: Train accuracy = {train_acc:.2f}% \"\n",
    "                  f\"[codon={xk[0]:.2f}, imm={xk[1]:.2f}, rbs={xk[2]:.2f}, length={xk[3]:.2f}, start={xk[4]:.2f}]\")\n",
    "    \n",
    "    result_train = differential_evolution(\n",
    "        objective_function_multi_genome,\n",
    "        bounds,\n",
    "        args=(all_genome_data, train_ids),\n",
    "        strategy='best1bin',\n",
    "        maxiter=100,\n",
    "        popsize=15,\n",
    "        tol=0.0001,\n",
    "        mutation=(0.5, 1),\n",
    "        recombination=0.7,\n",
    "        callback=callback_train,\n",
    "        polish=True,\n",
    "        workers=1,\n",
    "        updating='deferred'\n",
    "    )\n",
    "    \n",
    "    optimal_weights = {\n",
    "        'codon': result_train.x[0],\n",
    "        'imm': result_train.x[1],\n",
    "        'rbs': result_train.x[2],\n",
    "        'length': result_train.x[3],\n",
    "        'start': result_train.x[4]\n",
    "    }\n",
    "    \n",
    "    train_accuracy = -result_train.fun * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2: EVALUATING ON VALIDATION SET\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_score = objective_function_multi_genome(result_train.x, all_genome_data, val_ids)\n",
    "    val_accuracy = -val_score * 100\n",
    "    \n",
    "    print(f\"Validation accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Evaluate on individual validation genomes\n",
    "    print(\"\\nPer-genome validation results:\")\n",
    "    for gid in val_ids:\n",
    "        score = objective_function_multi_genome(result_train.x, all_genome_data, [gid])\n",
    "        acc = -score * 100\n",
    "        print(f\"  {gid}: {acc:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAIN/VALIDATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Training accuracy:   {train_accuracy:.2f}%\")\n",
    "    print(f\"Validation accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"Generalization gap:  {train_accuracy - val_accuracy:.2f}%\")\n",
    "    \n",
    "    if abs(train_accuracy - val_accuracy) < 2.0:\n",
    "        print(\"✓ Good generalization! Weights work across genomes.\")\n",
    "    else:\n",
    "        print(\"⚠ Possible overfitting - weights may be too genome-specific.\")\n",
    "    \n",
    "    print(\"\\nOptimal weights (from training):\")\n",
    "    for key, val in optimal_weights.items():\n",
    "        print(f\"  {key}: {val:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return optimal_weights, train_accuracy, val_accuracy\n",
    "\n",
    "\n",
    "def optimize_on_all_genomes(all_genome_data):\n",
    "    \"\"\"\n",
    "    OPTION B: Optimize on ALL genomes (no split).\n",
    "    Use this for final production weights after validation.\n",
    "    \"\"\"\n",
    "    genome_ids = list(all_genome_data.keys())\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTION B: OPTIMIZING ON ALL GENOMES\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Optimizing across {len(genome_ids)} genomes: {genome_ids}\")\n",
    "    print(\"This finds weights that work best on average across all organisms.\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    bounds = [\n",
    "        (0.1, 5.0),  # codon\n",
    "        (0.1, 10.0), # imm\n",
    "        (0.1, 5.0),  # rbs\n",
    "        (0.1, 10.0), # length\n",
    "        (0.1, 3.0),  # start\n",
    "    ]\n",
    "    \n",
    "    iteration = [0]\n",
    "    best_so_far = [float('inf')]\n",
    "    \n",
    "    def callback(xk, convergence):\n",
    "        iteration[0] += 1\n",
    "        current_score = objective_function_multi_genome(xk, all_genome_data, genome_ids)\n",
    "        if current_score < best_so_far[0]:\n",
    "            best_so_far[0] = current_score\n",
    "            avg_acc = -current_score * 100\n",
    "            print(f\"Iter {iteration[0]}: Avg accuracy = {avg_acc:.2f}% \"\n",
    "                  f\"[codon={xk[0]:.2f}, imm={xk[1]:.2f}, rbs={xk[2]:.2f}, length={xk[3]:.2f}, start={xk[4]:.2f}]\")\n",
    "    \n",
    "    result = differential_evolution(\n",
    "        objective_function_multi_genome,\n",
    "        bounds,\n",
    "        args=(all_genome_data, genome_ids),\n",
    "        strategy='best1bin',\n",
    "        maxiter=100,\n",
    "        popsize=15,\n",
    "        tol=0.0001,\n",
    "        mutation=(0.5, 1),\n",
    "        recombination=0.7,\n",
    "        callback=callback,\n",
    "        polish=True,\n",
    "        workers=1,\n",
    "        updating='deferred'\n",
    "    )\n",
    "    \n",
    "    optimal_weights = {\n",
    "        'codon': result.x[0],\n",
    "        'imm': result.x[1],\n",
    "        'rbs': result.x[2],\n",
    "        'length': result.x[3],\n",
    "        'start': result.x[4]\n",
    "    }\n",
    "    \n",
    "    avg_accuracy = -result.fun * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL-GENOME OPTIMIZATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Average accuracy: {avg_accuracy:.2f}%\")\n",
    "    \n",
    "    # Show per-genome breakdown\n",
    "    print(\"\\nPer-genome accuracy:\")\n",
    "    for gid in genome_ids:\n",
    "        score = objective_function_multi_genome(result.x, all_genome_data, [gid])\n",
    "        acc = -score * 100\n",
    "        print(f\"  {gid}: {acc:.2f}%\")\n",
    "    \n",
    "    print(\"\\nOptimal weights:\")\n",
    "    for key, val in optimal_weights.items():\n",
    "        print(f\"  {key}: {val:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return optimal_weights, avg_accuracy\n",
    "\n",
    "\n",
    "def combined_best_practice_optimization(TEST_GENOMES, cached_data):\n",
    "    \"\"\"\n",
    "    COMBINED BEST PRACTICE: \n",
    "    1. Prepare all genome data\n",
    "    2. Train/Val split to check generalization\n",
    "    3. If good generalization, train on all data for final weights\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\" \" * 30 + \"BEST PRACTICE WEIGHT OPTIMIZATION\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    print(\"\\nSTEP 1: PREPARING DATA\")\n",
    "    all_genome_data = prepare_all_genome_data(TEST_GENOMES, cached_data)\n",
    "    \n",
    "    # Step 2: Train/Val split\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"\\nSTEP 2: TRAIN/VALIDATION SPLIT\")\n",
    "    weights_trainval, train_acc, val_acc = optimize_with_train_val_split(all_genome_data)\n",
    "    \n",
    "    # Step 3: Decision point\n",
    "    generalization_gap = abs(train_acc - val_acc)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"\\nSTEP 3: FINAL WEIGHT SELECTION\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    if generalization_gap < 2.0:\n",
    "        print(\"✓ Good generalization detected!\")\n",
    "        print(\"  Training on ALL genomes for final production weights...\")\n",
    "        print()\n",
    "        weights_final, avg_acc = optimize_on_all_genomes(all_genome_data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"FINAL RECOMMENDATION: Use ALL-GENOME weights\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Average accuracy: {avg_acc:.2f}%\")\n",
    "        print(\"\\nFinal weights:\")\n",
    "        for key, val in weights_final.items():\n",
    "            print(f\"  {key}: {val:.4f}\")\n",
    "        \n",
    "        return weights_final, avg_acc\n",
    "    else:\n",
    "        print(\"⚠ Generalization gap is significant\")\n",
    "        print(\"  Recommend using train/val weights or investigating overfitting\")\n",
    "        print()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"FINAL RECOMMENDATION: Use TRAIN/VAL weights (more conservative)\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Training accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Validation accuracy: {val_acc:.2f}%\")\n",
    "        print(\"\\nWeights:\")\n",
    "        for key, val in weights_trainval.items():\n",
    "            print(f\"  {key}: {val:.4f}\")\n",
    "        \n",
    "        return weights_trainval, val_acc\n",
    "\n",
    "\n",
    "final_weights, final_accuracy = combined_best_practice_optimization(TEST_GENOMES, cached_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b773e",
   "metadata": {},
   "source": [
    "# FILTERING THRESHOLDS OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cde3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_genome_data_with_scores(genome_list, cached_data, weights):\n",
    "    \"\"\"\n",
    "    Pre-compute scored ORFs (we defined this earlier).\n",
    "    \"\"\"\n",
    "    from src.comparative_analysis import get_gff_path\n",
    "    \n",
    "    all_genome_data = {}\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"PREPARING SCORED GENOME DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for gid in genome_list:\n",
    "        print(f\"\\nProcessing {gid}...\")\n",
    "        \n",
    "        genome_data = cached_data[gid]\n",
    "        all_orfs = genome_data['orfs']\n",
    "        \n",
    "        training_set = create_training_set(gid, cached_data)\n",
    "        intergenic_set = create_intergenic_set(gid, cached_data)\n",
    "        models = build_all_scoring_models_cached(training_set, intergenic_set)\n",
    "        \n",
    "        scored_orfs = score_all_orfs_cached(all_orfs, models)\n",
    "        scored_orfs = normalize_all_orf_scores(scored_orfs)\n",
    "        scored_orfs = add_combined_scores(scored_orfs)\n",
    "        \n",
    "        gff_path = get_gff_path(gid)\n",
    "        ref = pd.read_csv(gff_path, sep=\"\\t\", comment=\"#\", header=None)\n",
    "        ref_genes = ref[ref[2] == \"CDS\"][[3, 4]]\n",
    "        ref_genes.columns = [\"start\", \"end\"]\n",
    "        ref_set = set(zip(ref_genes['start'], ref_genes['end']))\n",
    "        \n",
    "        all_genome_data[gid] = {\n",
    "            'scored_orfs': scored_orfs,\n",
    "            'ref_set': ref_set\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ {len(scored_orfs)} ORFs scored\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Total genomes prepared: {len(all_genome_data)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return all_genome_data\n",
    "\n",
    "def optimize_ALL_FOUR_THRESHOLDS(TEST_GENOMES, cached_data, fixed_weights):\n",
    "    \"\"\"\n",
    "    Optimize ALL 4 thresholds: codon, imm, length, combined (8 parameters total).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\" \" * 15 + \"OPTIMIZING ALL 4 THRESHOLDS (8 parameters)\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    genome_subset = ['NC_000913.3', 'NC_000964.3', 'NC_002505.1']\n",
    "    \n",
    "    print(\"\\nSTEP 1: PREPARING DATA\")\n",
    "    genome_data = prepare_genome_data_with_scores(genome_subset, cached_data, fixed_weights)\n",
    "    genome_ids = list(genome_data.keys())\n",
    "    \n",
    "    def objective_function(threshold_array, genome_data_dict, genome_ids, weights):\n",
    "        \"\"\"\n",
    "        8 parameters: [init_codon, init_imm, init_length, init_combined,\n",
    "                       final_codon, final_imm, final_length, final_combined]\n",
    "        \"\"\"\n",
    "        init_codon, init_imm, init_length, init_combined = threshold_array[:4]\n",
    "        final_codon, final_imm, final_length, final_combined = threshold_array[4:]\n",
    "        \n",
    "        def weighted_score(orf):\n",
    "            return (\n",
    "                orf['codon_score_norm'] * weights['codon'] +\n",
    "                orf['imm_score_norm'] * weights['imm'] +\n",
    "                orf['rbs_score_norm'] * weights['rbs'] +\n",
    "                orf['length_score_norm'] * weights['length'] +\n",
    "                orf['start_score_norm'] * weights['start']\n",
    "            )\n",
    "        \n",
    "        total_tp = 0\n",
    "        total_fp = 0\n",
    "        total_fn = 0\n",
    "        \n",
    "        for gid in genome_ids:\n",
    "            scored_orfs = genome_data_dict[gid]['scored_orfs']\n",
    "            ref_set = genome_data_dict[gid]['ref_set']\n",
    "            \n",
    "            # Initial filter with ALL 4 thresholds\n",
    "            candidates = []\n",
    "            for orf in scored_orfs:\n",
    "                length_score = orf.get('length_score', 0)\n",
    "                codon_score = orf.get('codon_score', 0)\n",
    "                imm_score = orf.get('imm_score', 0)\n",
    "                combined_score = orf.get('combined_score', 0)\n",
    "                \n",
    "                all_three_below = (length_score < init_length and \n",
    "                                  codon_score < init_codon and \n",
    "                                  imm_score < init_imm)\n",
    "                combined_below = combined_score < init_combined\n",
    "                \n",
    "                if not (all_three_below or combined_below):\n",
    "                    candidates.append(orf)\n",
    "            \n",
    "            # Group and select starts\n",
    "            grouped = organize_nested_orfs(candidates)\n",
    "            top_candidates = []\n",
    "            for stop_pos, orf_group in grouped.items():\n",
    "                if len(orf_group) == 1:\n",
    "                    top_candidates.append(orf_group[0])\n",
    "                else:\n",
    "                    best = max(orf_group, key=weighted_score)\n",
    "                    top_candidates.append(best)\n",
    "            \n",
    "            # Final filter with ALL 4 thresholds\n",
    "            final_predictions = []\n",
    "            for orf in top_candidates:\n",
    "                length_score = orf.get('length_score', 0)\n",
    "                codon_score = orf.get('codon_score', 0)\n",
    "                imm_score = orf.get('imm_score', 0)\n",
    "                combined_score = orf.get('combined_score', 0)\n",
    "                \n",
    "                all_three_below = (length_score < final_length and \n",
    "                                  codon_score < final_codon and \n",
    "                                  imm_score < final_imm)\n",
    "                combined_below = combined_score < final_combined\n",
    "                \n",
    "                if not (all_three_below or combined_below):\n",
    "                    final_predictions.append(orf)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            for orf in final_predictions:\n",
    "                coord = (orf.get('genome_start', orf['start']), \n",
    "                        orf.get('genome_end', orf['end']))\n",
    "                if coord in ref_set:\n",
    "                    total_tp += 1\n",
    "                else:\n",
    "                    total_fp += 1\n",
    "            \n",
    "            predicted_set = {(orf.get('genome_start', orf['start']), \n",
    "                             orf.get('genome_end', orf['end']))\n",
    "                            for orf in final_predictions}\n",
    "            total_fn += len(ref_set - predicted_set)\n",
    "        \n",
    "        sensitivity = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        \n",
    "        # Geometric mean - requires BOTH high\n",
    "        geometric_mean = (sensitivity * precision) ** 0.5\n",
    "        \n",
    "        # Penalties for low values\n",
    "        penalty = 0\n",
    "        if sensitivity < 0.78:\n",
    "            penalty += 10 * (0.78 - sensitivity)\n",
    "        if precision < 0.60:\n",
    "            penalty += 10 * (0.60 - precision)\n",
    "        \n",
    "        return -(geometric_mean - penalty)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"STEP 2: OPTIMIZING ALL 8 THRESHOLD PARAMETERS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Parameters: [init_codon, init_imm, init_length, init_combined,\")\n",
    "    print(\"             final_codon, final_imm, final_length, final_combined]\")\n",
    "    print()\n",
    "    \n",
    "    # Bounds for ALL 4 thresholds × 2 filters = 8 parameters\n",
    "    bounds = [\n",
    "        (-0.3, 0.3),   # init_codon_threshold\n",
    "        (-0.3, 0.3),   # init_imm_threshold\n",
    "        (-0.3, 0.3),   # init_length_threshold\n",
    "        (-0.3, 0.2),   # init_combined_threshold\n",
    "        (0.0, 0.5),    # final_codon_threshold\n",
    "        (0.0, 0.5),    # final_imm_threshold\n",
    "        (0.0, 0.5),    # final_length_threshold\n",
    "        (0.1, 0.5),    # final_combined_threshold\n",
    "    ]\n",
    "    \n",
    "    iteration = [0]\n",
    "    best_geom = [0]\n",
    "    \n",
    "    def callback(xk, convergence):\n",
    "        iteration[0] += 1\n",
    "        current_score = -objective_function(xk, genome_data, genome_ids, fixed_weights)\n",
    "        if current_score > best_geom[0]:\n",
    "            best_geom[0] = current_score\n",
    "            print(f\"Iter {iteration[0]}: Score={current_score:.4f}\")\n",
    "            print(f\"  Init:  codon={xk[0]:.3f}, imm={xk[1]:.3f}, length={xk[2]:.3f}, comb={xk[3]:.3f}\")\n",
    "            print(f\"  Final: codon={xk[4]:.3f}, imm={xk[5]:.3f}, length={xk[6]:.3f}, comb={xk[7]:.3f}\")\n",
    "    \n",
    "    from scipy.optimize import differential_evolution\n",
    "    \n",
    "    print(\"Starting optimization...\")\n",
    "    print()\n",
    "    \n",
    "    result = differential_evolution(\n",
    "        objective_function,\n",
    "        bounds,\n",
    "        args=(genome_data, genome_ids, fixed_weights),\n",
    "        strategy='best1bin',\n",
    "        maxiter=50,      # More iterations for 8 params\n",
    "        popsize=15,      # Larger population for 8 params\n",
    "        tol=0.001,\n",
    "        mutation=(0.5, 1),\n",
    "        recombination=0.7,\n",
    "        callback=callback,\n",
    "        polish=True,\n",
    "        workers=1,\n",
    "        updating='deferred'\n",
    "    )\n",
    "    \n",
    "    optimal_thresholds = {\n",
    "        'initial_filter': {\n",
    "            'codon_threshold': result.x[0],\n",
    "            'imm_threshold': result.x[1],\n",
    "            'length_threshold': result.x[2],\n",
    "            'combined_threshold': result.x[3]\n",
    "        },\n",
    "        'final_filter': {\n",
    "            'codon_threshold': result.x[4],\n",
    "            'imm_threshold': result.x[5],\n",
    "            'length_threshold': result.x[6],\n",
    "            'combined_threshold': result.x[7]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ALL-THRESHOLD OPTIMIZATION COMPLETE!\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Best Score (geometric mean): {best_geom[0]:.4f}\")\n",
    "    print(\"\\nOptimal Initial Filter:\")\n",
    "    print(f\"  codon_threshold    = {optimal_thresholds['initial_filter']['codon_threshold']:.4f}\")\n",
    "    print(f\"  imm_threshold      = {optimal_thresholds['initial_filter']['imm_threshold']:.4f}\")\n",
    "    print(f\"  length_threshold   = {optimal_thresholds['initial_filter']['length_threshold']:.4f}\")\n",
    "    print(f\"  combined_threshold = {optimal_thresholds['initial_filter']['combined_threshold']:.4f}\")\n",
    "    print(\"\\nOptimal Final Filter:\")\n",
    "    print(f\"  codon_threshold    = {optimal_thresholds['final_filter']['codon_threshold']:.4f}\")\n",
    "    print(f\"  imm_threshold      = {optimal_thresholds['final_filter']['imm_threshold']:.4f}\")\n",
    "    print(f\"  length_threshold   = {optimal_thresholds['final_filter']['length_threshold']:.4f}\")\n",
    "    print(f\"  combined_threshold = {optimal_thresholds['final_filter']['combined_threshold']:.4f}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    return optimal_thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "17ca4dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "               OPTIMIZING ALL 4 THRESHOLDS (8 parameters)\n",
      "====================================================================================================\n",
      "\n",
      "STEP 1: PREPARING DATA\n",
      "================================================================================\n",
      "PREPARING SCORED GENOME DATA\n",
      "================================================================================\n",
      "\n",
      "Processing NC_000913.3...\n",
      "Building traditional scoring models...\n",
      "  Building codon usage models...\n",
      "  Building IMM models...\n",
      "✓ All models built in 2.5s\n",
      "  IMM order: 6\n",
      "  Training sequences: 1220 (1,809,003 bp)\n",
      "  Intergenic sequences: 490 (148,082 bp)\n",
      "Scoring 176,315 ORFs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m optimal_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4.8562\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0107\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2755\u001b[39m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Optimize ALL 4 thresholds (8 parameters)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m complete_thresholds \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_ALL_FOUR_THRESHOLDS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_GENOMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[93], line 57\u001b[0m, in \u001b[0;36moptimize_ALL_FOUR_THRESHOLDS\u001b[1;34m(TEST_GENOMES, cached_data, fixed_weights)\u001b[0m\n\u001b[0;32m     54\u001b[0m genome_subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNC_000913.3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNC_000964.3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNC_002505.1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSTEP 1: PREPARING DATA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m genome_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_genome_data_with_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenome_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m genome_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(genome_data\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective_function\u001b[39m(threshold_array, genome_data_dict, genome_ids, weights):\n",
      "Cell \u001b[1;32mIn[93], line 23\u001b[0m, in \u001b[0;36mprepare_genome_data_with_scores\u001b[1;34m(genome_list, cached_data, weights)\u001b[0m\n\u001b[0;32m     20\u001b[0m intergenic_set \u001b[38;5;241m=\u001b[39m create_intergenic_set(gid, cached_data)\n\u001b[0;32m     21\u001b[0m models \u001b[38;5;241m=\u001b[39m build_all_scoring_models_cached(training_set, intergenic_set)\n\u001b[1;32m---> 23\u001b[0m scored_orfs \u001b[38;5;241m=\u001b[39m \u001b[43mscore_all_orfs_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_orfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m scored_orfs \u001b[38;5;241m=\u001b[39m normalize_all_orf_scores(scored_orfs)\n\u001b[0;32m     25\u001b[0m scored_orfs \u001b[38;5;241m=\u001b[39m add_combined_scores(scored_orfs)\n",
      "Cell \u001b[1;32mIn[89], line 165\u001b[0m, in \u001b[0;36mscore_all_orfs_cached\u001b[1;34m(all_orfs, models)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Score each component\u001b[39;00m\n\u001b[0;32m    159\u001b[0m orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodon_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m score_codon_bias_ratio(\n\u001b[0;32m    160\u001b[0m     orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m    161\u001b[0m     codon_model, \n\u001b[0;32m    162\u001b[0m     background_codon_model\n\u001b[0;32m    163\u001b[0m )\n\u001b[1;32m--> 165\u001b[0m orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimm_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mscore_imm_ratio_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43morf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoding_imm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoncoding_imm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_order\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Length score\u001b[39;00m\n\u001b[0;32m    172\u001b[0m orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m score_orf_length(orf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[89], line 106\u001b[0m, in \u001b[0;36mscore_imm_ratio_cached\u001b[1;34m(sequence, coding_imm, noncoding_imm, max_order)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sequence)):\n\u001b[0;32m    104\u001b[0m     nucleotide \u001b[38;5;241m=\u001b[39m sequence[i]\n\u001b[1;32m--> 106\u001b[0m     context_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_order\u001b[49m)\n\u001b[0;32m    107\u001b[0m     context \u001b[38;5;241m=\u001b[39m sequence[context_start:i]\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_frame_aware:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# RUN IT!\n",
    "optimal_weights = {\n",
    "    'codon': 4.8562,\n",
    "    'imm': 1.0107,\n",
    "    'rbs': 0.6383,\n",
    "    'length': 7.4367,\n",
    "    'start': 0.2755\n",
    "}\n",
    "\n",
    "# Optimize ALL 4 thresholds (8 parameters)\n",
    "complete_thresholds = optimize_ALL_FOUR_THRESHOLDS(TEST_GENOMES, cached_data, optimal_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
